[[{"l":"Empowering Digital Resilience Through Operationalizing Data Analytics Methodology","p":["ODAM, or Operationalizing Data Analytics Methodology, is a comprehensive approach that helps organizations build a data-driven culture and achieve measurable business outcomes. It provides a framework for defining, planning, executing, and measuring data analytics initiatives, ensuring that they align with strategic goals and maximize the value of data assets. With ODAM, organizations can mature and operationalize their data analytics capabilities, driving business value and supporting decision-making across departments and use cases. By following best practices and leveraging industry-leading tools and technologies, organizations can accelerate progress and lead the way in data-driven innovation."]},{"l":"The Work","p":["This work is an experiment in writing that combines research, opinion, personal experiences, poetry, storytelling, history, hypotheticals, and interviews. The goal is to bring together a variety of ideas and present them in a way that is helpful and actionable to readers in their daily lives. Throughout my career, I have worked in customer service roles and learned that providing exceptional service is both memorable for customers and cost-effective for businesses. With this work, I hope to not only provide excellent customer service, but also to raise the bar for what customers should expect from vendors.","This work focuses on the intersection of data and community, and aims to help organizations of all kinds build an Analytics Center of Excellence (ACE) and to operationalize an organization’s data to drive value from data.","My passion for this work comes from a desire to help customers be successful with technology, as well as my interest in how people and machines interact and connect. I am fascinated by the ways in which different groups of people and devices communicate and work together, and the intersection between humans and machines.","In this writing, we will explore how organizations of all sizes can remove the barriers between data and action while building out their own ACE, using Splunk as a data analytics and visualization platform. We will delve into topics such as aligning expectations, planning, and implementation, with the goal of helping users access the right data insights at the right time. By taking a practical, in-the-moment approach to data analytics, organizations can empower their users to make the most of their data.","There are a series of valves, gates, and other mechanisms to help get water from one place to another. I took this photo in Doña Ana, New Mexico, 2022.","Thank you for joining me on this journey. In the following pages, we will explore how your organization can operationalize data analytics. But first, let me share a bit more about my endearment for data."]}],[{"l":"Foreward","p":["As a Chief Information Security Officer for a major research university, it can be frustrating to know that the clues to imminent security breaches are within reach, but not easily accessible. Analysts work tirelessly to construct a timeline of what, how, and when a breach occurred, but by the time they have the information, the damage has already been done. Sensitive data is stolen, important information disclosed, and critical operations have been shut down by threat actors. What's even more disheartening is that the majority of breaches result from simple phishing attacks where users' credentials are compromised. Other breaches involve privilege escalation, where once an attacker has gained access to an environment, they increase their level of access to install malware or exfiltrate data.","Why do common attacks continue to succeed against organizations?","We revel in the thrill of investigating a major attack, thirsting to comprehend its scope, and engaging in hand-to-hand combat with the attacker as they attempt to evade the containment measures set in place by the response team. The adrenaline rush is palpable as we work to uncover the details and restore order.","This is an exciting scenario, like the cool scenes in movies where the hero is under immense pressure to save the world with limited time. They swiftly uncover the culprit, decipher their method, and promptly boot them out of the network, reclaiming control of the system making everything right again.","During security incidents and investigations, teams from various departments and management levels within the organization come together - IT, Legal, Communications, Functional Service Owners, Subject Matter Experts, and Executives - to contain the incident and bring things back to normal. Despite their hard work and efforts, once normalcy is restored, there is often a lack of urgency to switch to prevention mode with the same drive and collaboration. Instead, the organization inadvertently becomes passive, waiting for the next incident to happen. Sadly though, while the wait is on, the data is there trying to tell the story that the next incident has already begun, we just cannot see it yet - we are not looking!","The ODAM (Operationalizing Data Analytics Methodology) promises a framework with repeatable processes for translating and illuminating the story that the data around us is trying to communicate, so that positive actions can be taken to defend our digital assets and capabilities. ODAM promises to take away the common excuses for not leveraging cybersecurity data analytics in the battle to prevent successful attacks and to quickly recover with resilience should a breach occur."]},{"i":"excuse-1","l":"Excuse #1","p":["Siloed teams and lack of understanding of the IT environment hampers effective data analytics.","The absence of familiarity with the IT landscape hinders one's ability to distinguish crucial data sources that carry relevance to an operation. Take, for instance, the typical scenario of a central office user accessing the corporate Enterprise Resource Planning (ERP) system. Vital system elements along the connection route offer insights on security vulnerabilities, potential threats, or oncoming assaults. Yet, a comprehensive understanding of these elements is imperative for executing data analytics that effectively narrate the unfolding events and their probable future occurrences.","The ODAM framework advocates for an IT blueprint workshop, where subject matter experts from diverse silos convene to chart the service landscape and pinpoint crucial data sources and their significance. In our ERP scenario (refer to figure below), this involves subject matter specialists from the networking, systems, application development & support, identity & access management, and other relevant teams coming together to delineate the precise connection of a central office user to the corporate ERP via the Internet.","With this newfound insight into the system components involved in connecting a central office user to the corporate ERP system, discussions can be held to determine the relevance and significance of logs and/or transactional data from each component for the purpose of cybersecurity data analytics. Other teams, including those in networking, application development and support, and systems, will come to understand that the benefits of this effort extend beyond cybersecurity and into the realm of general observability. The logs generated by systems, applications, and security tools are not limited to use in cybersecurity data analytics, but can also provide valuable performance information that can be used by these teams to uphold their service level agreements. As a result, discussions about the convergence of network operations centers (NOCs) and security operation centers (SOCs) become even more meaningful."]},{"i":"excuse-2","l":"Excuse #2","p":["Talent shortage and cybersecurity staff are not skilled in data analytics.","The ODAM framework follows a systematic approach to map out all system components involved in various common use cases such as a user connecting to the Internet, accessing email, or an administrator connecting to a backend system. By doing so, the team can work together to locate and incorporate the relevant data sources into the cybersecurity analytics environment. Today's leading analytics platforms offer robust capabilities for visualizing large datasets, essential for SOC analysts, CISOs, and other organization leaders. The advantage of the ODAM framework is its clear and repeatable process for operationalizing data analytics for any use case, to increase efficiency and expertise. It's estimated that after completing three to five use cases, the cyber team should be able to continue identifying and operationalizing data analytics for two to three additional use cases per month. Through proper training and management, the challenges of a talent shortage and under-skilled staff in the data analytics field can be overcome."]},{"i":"excuse-3","l":"Excuse #3","p":["Data is expensive.","The ODAM framework takes a more strategic approach to developing an ACE (Analytics Center of Excellence) by focusing on use cases and mapping out the relevant system components, rather than collecting as much data as possible, which can often prove to be very costly. Just like in gold mining, not all dirt is pay-dirt, similarly for data mining, not all data will provide value to an organization's decision-making capabilities. The goal is to identify and connect the data sources that will provide the most value to the organization's decision analytics capabilities. This approach helps to reduce costs associated with storing and managing irrelevant data, freeing up resources for more valuable data and activities. By focusing on use cases and identifying the right data sources, organizations can optimize their data collection efforts and avoid wasting resources on over-collecting data that may not provide value.","The ODAM framework provides an alternative method for creating your ACE, guided by use cases that align with your organization's objectives. The framework suggests a structured process for collecting only the data needed to support the analytics required for each use case, as the data required for different use cases are likely to overlap. This helps to minimize data collection by disregarding unnecessary data and focusing on sources that provide significant statistical insights into your ACE's activities. The use-case driven approach results in more predictable budgeting compared to the costly \"collect everything\" approach and enables regular reassessment of data source requirements as business needs change.","Meteorology vs Archaeology in the SOC","Many security operations centers (SOCs) today function like archaeological digging sites, constantly searching through millions of logs for clues to past incidents to understand the vulnerability that was exploited, how the attacker entered the environment, how to contain the incident, and how to remove the attacker and their payload to get back to normal operations. These are important questions that can be answered through SIEM tools and other security tools in the SOC. However, I aim for a meteorological approach to cybersecurity data analytics, one that allows me to predict future cyber incidents and take preventative measures or to be ready to respond and recover in the event of a successful attack. The ODAM framework rekindles my hope for predictive analytics and forecasting of cybersecurity incidents. While the framework doesn't guarantee this outcome, it provides a roadmap to set up an ACE that, if implemented correctly, could pave the way for this type of analytics."]},{"i":"a-cisos-ultimate-desire-for-odam","l":"A CISO’s ultimate desire for ODAM?"},{"i":"desire-1","l":"Desire #1","p":["Forecast Risk","As a CISO, the ultimate goal is to know the risk of the next major incident occurring (RI), to be able to stop it or adequately prepare to respond and recover. Consider this simple relationship between Risk of an incident (RI), existing vulnerabilities (VI), the value of the asset that will be affected during the incident (AI):","From a mathematical perspective, the risk of an incident can be predicted if the threat to an asset is known, vulnerability information is readily accessible, and the value of the asset to the organization is understood. This is the ultimate goal of the ODAM process, to use past and present events, along with sound cyber data analytics, to forecast future incidents. Threat data can be sourced from various internal and external threat intelligence sources, and by normalizing the right data sources and incorporating them into the ACE, we can align with the scenario-based approach proposed by the ODAM framework. Vulnerability data is available from internal and external vulnerability scanners, technology vendors, and UBA (user behavioral analytics) platforms. On the other hand, the information about assets is more complex, given the large number of assets on major organizations' networks, and their multi-dimensional interrelationships and interdependencies.","The asset is of paramount importance, but its complexity surpasses our understanding of threats and vulnerabilities. Threats are an inevitable reality, always lurking and waiting to strike. Nevertheless, their impact can be mitigated by acquiring solid threat intelligence as soon as possible and converting it into actions for the SOC (security operations center). Vulnerabilities, on the other hand, are somewhat within our grasp, as they can be eliminated or addressed by implementing a proper patching, scanning, and management program. Yes, zero-day vulnerabilities exist, but a robust vulnerability management program still positions an organization to handle them better when they arise. If an organization has a mature program in place to address vulnerabilities, adding another one is just a matter of course. However, if a zero-day occurs amid hundreds of unaddressed vulnerabilities, that critical patch becomes harder to install, as several other versions may need to be applied first.","Assets are at the heart of the risk equation and the most complex aspect to grasp. To prioritize and effectively understand the potential for an asset to be compromised, one must have a thorough understanding of not only the value it holds for the organization, but also a multitude of other factors.","When evaluating the risks associated with an asset, it's crucial to consider not only its value to the organization but also a range of other factors that could increase the likelihood of it being compromised. These factors include:"]},{"l":"Asset Value","p":["The worth of an asset depends on its functions or capabilities. For instance, an ERP or CRM system may hold sensitive information about employees, customers, and suppliers, which could give the organization a competitive advantage. Meanwhile, a nurse's station may be critical in providing patient care during emergencies. To prioritize and effectively weigh the risks, each organization needs to determine the direct value of the asset or the value it enables in other assets."]},{"l":"Interactions with other Assets","p":["The ODAM framework focuses on enumerating the interactions between assets in an organization, which helps to establish a baseline of normal interactions and identify anomalies that could indicate ongoing or imminent attacks."]},{"l":"User Interactions with Assets","p":["User-asset interactions provide valuable data that can be included in the organization's ACE. This baseline data can be used to detect anomalies and automatically or manually address them through the SOC.","The defense mechanism around an asset is crucial in applying the risk equation. In the event that defense fails, prompt detection is vital to prevent the successful exploitation of automated attacks that are constantly targeting an organization's network. This is where the ODAM framework comes in handy, providing visibility into the status of defense components and enabling prioritization of response actions based on other ongoing events in the environment.","The ultimate objective of the ODAM framework is to simplify the processes involved in building an effective cybersecurity ACE by bringing together the various components of the risk equation in a methodical and incremental manner, making it easier for cybersecurity professionals and other relevant stakeholders to understand and manage the risks they face."]},{"i":"desire-2","l":"Desire #2","p":["Have meaningful and actionable metrics and dashboards","In my opinion, the most impactful way to convey data is through the use of clear and concise visuals and metrics. The ODAM framework aims to simplify the process of transforming data into compelling narratives that can be communicated through the use of visual aids such as graphs, dashboards, metrics, and key performance indicators. The objective is to make sure that the information is easily accessible and understandable by all stakeholders, regardless of their role or level of expertise within the organization.","My ideal scenario would be for the president of an organization to simply glance at the daily dashboard and understand the current state of cybersecurity. If the dashboard displays a green color, it's a good day, and no major incidents are expected in the near future. Conversely, if the dashboard is red, the president knows that a significant breach is imminent and immediate action must be taken. In this scenario, the line worker would also have access to the same dashboard, albeit with varying levels of detail, and work proactively to mitigate the risk. In the end, when the CISO is called to the boardroom, they are able to respond with confidence, saying: \"We have a problem, but we know how to fix it, and this is what it will take to do so.\"","The ODAM framework strives towards standardizing cybersecurity metrics, which is currently hindered by the lack of uniformity. Unlike financial metrics like EPS (earnings per share) or FPE (forward price to earnings ratio), which are well-known and widely accepted performance indicators of a company’s finance, the field of cybersecurity lacks consistency in metrics. The ODAM framework offers a path to solving this problem, by providing a methodical approach to analyzing data, determining relevant performance indicators, and creating common metrics that have the same meaning across organizations. This could allow executives to have a clear understanding of their organization's cybersecurity performance and its resilience against incoming attacks. Furthermore, with the right data and tooling in the ACE, it will pave the way for creating metrics and KPIs that can be easily included in a company's performance statistics and understood by a wider audience, just like financial metrics like EPS or FPE ratios."]},{"i":"odam-a-new-day","l":"ODAM, a New Day","p":["CISOs are always searching for ways to grasp the immense amount of data that surrounds us and reveal the story it tells about the robustness of our defenses. However, there is a persistent challenge in balancing the attention and resources given to both incident response, containment, and recovery, and to planning and implementing preventative measures. Despite being aware of the crucial role that data analytics plays in a successful cybersecurity strategy, many organizations still face obstacles such as IT silos, a shortage of skilled personnel, and cost concerns, resulting in a lack of focus and inadequate investment in this area. Presently, many security operation centers are akin to archaeological excavation sites, where analysts sift through millions of logs for days on end to uncover what has already occurred, just as archaeologists uncover historical secrets. CISOs aspire for a meteorological approach to cybersecurity data analytics, where the emphasis shifts to forecasting future cyber incidents in a manner similar to a meteorologist's weather predictions.","The ODAM framework envisions a future where cybersecurity data analytics is no longer a challenge for CISOs, but a solution. It promises to simplify the complex process of bringing together the different components of risk - assets, vulnerabilities, and threats - through a methodical and incremental approach to building an effective cybersecurity ACE. With the ODAM framework, the barriers to leveraging cybersecurity data analytics, such as IT siloes, talent shortages, and cost, can be overcome. The ultimate goal of a CISO is to understand the risk of a potential incident, prevent it and/or be prepared to respond and recover. The framework aims to make this goal a reality by offering a repeatable process for turning data into actionable stories through the use of visual aids, dashboards, metrics, and KPIs. The right data and tools in your ACE should enable the creation of metrics and KPIs that are as widely understood as a company's EPS or FPE ratios, providing a clear and concise representation of the organization's cybersecurity performance."]},{"l":"About the Author","p":["Leo Howell, Chief Information Security Officer Georgia Institute of Technology Office of Information Technology 756 West Peachtree Street NW Atlanta, Georgia 30308 phone: 404.385.0334 email: leo.howell@gatech.edu","Leo Howell is a visionary information security leader who is passionate about the \"I\" in IT as he believes that data leveraged as a strategic asset is a major competitive benefit to any organization. Leo has over 25 years of service in IT and currently serves as the chief information security officer for Georgia Institute of Technology where he practices his balanced approach to cybersecurity - stop the “bad” guys and empower our stakeholders to carry out the organization’s mission. Previously, Leo served as the chief information security officer for the University of Oregon and before that multiple leadership roles in security and audit at NC State University. Leo received his B.Sc. in Computer Science and Electronics from the University of the West Indies, Jamaica, and his MBA from NC State University. Leo is a Certified Information Security Professional (CISSP), Certified Information Systems Auditor (CISA) and a proud member of the international honor society Beta Gamma Sigma."]}],[{"l":"Data is Beautiful"},{"i":"spring-by-jimmy-santiago-baca","l":":icon-sponsor-tiers: Spring, by Jimmy Santiago Baca","p":["See Video","With one might tug and push, conservancy engineers in North New Mexico open water gates--snow melt, brimming northern lakes, and streams gush, lunge, and hurl down Río Abajo to community fields, fill the dry ditches and canals, clashing like great banging orchestra cymbals against dirt.","Plants, bushes, weeds uncurl, furl out along my ditch, ants float on islands of leaves, past pyramids of beer cans, tadpoles fuse to the water in swirls of brown mist, catfish fin beneath driftwood and stew the water dark brown.","All things pull and strain. The ditch swells into a great marketplace, where death and life are exchanged.","A pair of light blue-gray doves skim water surface, geese veer down, royal couriers flap-landing in gusty sprinkles, then serenely floating like white flags of peace, drifting in pairs, through glare and shadows, as crawdads, spiders, snakes and frogs peer from mud and weed corners.","From each unfolding lilac leaf a blue-green arctic haze glimmers, from feathers dark winter melts, from eyes glide out cold deeps, everything bears a new light.","The crane is breaking ice with its call, long-legged spider skitters to birth-thawing rhythms of the shore, as spring finally arrives, glistening the dead-slag of winter in all creatures, as they emit that special light they do.","— Jimmy Santiago Baca"]},{"l":"Navigation","p":["Exploring the Value of Data","Bringing Data to Life","Understanding the Hierarchy of Data Value","The Role of Data Analytics in Transforming Decision Making Process","Storytellilng with Data","Data at the Heart of Modern Life","Data-Driven Digital Organizations","Introducing Operationalizing Data Analytics Methodology","The Problems ODAM is Attempting to Solve"]}],[{"l":"Exploring the Value of Data","p":["A Journey Through the Wilderness","\"The universe is wider than our views of it.\"— Henry David Thoreau","Data is a powerful tool for conveying ideas, just as words can tell a story or art can depict the world. Splunk is software that helps people understand and make sense of their data.","This data story begins at the Dripping Springs trailhead in the Organ Mountains in Southern New Mexico, which recently became a protected national monument. Before setting out on my hike, I used my phone to check the weather forecast and made sure to pack my rain jacket. As I started my hike, I wore my Apple Watch, which tracked my GPS location, blood oxygen levels, heart rate, and other telemetry. My phone and watch provided me with real-time information about the weather and my location as I hiked. When it began to rain, I consulted my phone's weather radar app and decided to continue based on the information it provided and my own observations.","The desert smells wonderful when it rains. The hot ground is cooled as rain drops evaporate. The hike was enjoyable, and my trail app showed me that I had traveled a total distance of 4.95 miles in 2.5 hours, climbed 62 flights of stairs, and burned 1440 calories. It also recorded my heart rate and oxygen levels at 5-minute intervals throughout the hike. This is just one example of the vast amount of data that can be generated and consumed in a single activity.","All of the devices I had with me, including my phone, smartwatch, headphones, and even my AirPods, were capable of generating and streaming data. Two of these devices, my phone and watch, were also connected to cellular networks and had GPS capabilities. These devices provided me with valuable information when I needed it and also captured my personal biometric data, giving me insights into how my body responded to the hike. Let's look at a few more examples of how data and insights can be useful. Some examples of data that were valuable to me include:","Weather data: This was useful to me before and during my hike, but would not be valuable to me a week after the hike.","Location information: This was useful to me in real-time to show my exact location on a map and also valuable to me after the hike so I could look back at where I had previously hiked.","Health information: This is valuable to me when aggregated with other data, as it allows me to see trends over time and track my fitness metrics.","Geolocation information embedded in photos: This will be valuable to me when I want to filter my photos by location.","It's important to note that not all data is valuable after the fact. In my case, an incredible amount of data was generated, collected, and analyzed during my hike, and this data was valuable to different people and at different times. For example, the data collected by my devices was useful to me when I wanted to track my fitness or explore my hiking routes, but it might not be as valuable to someone else.","It's also worth considering that even when our devices are turned off, data is still being generated. The absence of data can be a data point in itself. For example, a cellular provider might be interested in knowing when 1% or more of all connected cell phones disconnect and stop sending data.","At the center of all this is me, the consumer of insights and the generator of data through my use of devices. As I use these devices for different activities and tasks, I create patterns and routines that are unique to me, and the way I navigate digital spaces becomes a data point in itself. Ultimately, I believe that data is evidence of our relationship with machines.","\"Maybe stories are just data with a soul.\"— Brené Brown Research Professor and Author"]}],[{"l":"Bringing Data to Life","p":["A Practical Approach to Building an Analytics Center of Excellence","Data is a valuable resource, just like water is in arid regions like New Mexico. In the same way that farmers must carefully manage their water rights and usage through contracts and agreements, organizations must also carefully manage their data to get the most value out of it. One way to do this is by building an Analytics Center of Excellence (ACE), which can help an organization make the most of its data by providing a central hub for data analytics and visualization.","The poem \"Spring\" by Jimmy Santiago Baca describes the arrival of spring in North New Mexico, as water flows into the acequia and brings life to the community fields. The acequia (ə-ˈsā-kē-ə), or network of canals, acts as a conduit for water, just as networks of data conduits can help organizations distribute insights and information to users.","In the same way that the acequia association helps to manage water rights and ensure the smooth bureaucratic flow of water to the fields, an ACE can help an organization manage data rights and ensure the smooth flow of data insights to users. The ACE can serve as a resource for answering questions and addressing issues related to data analytics."]},{"l":"Aspirations of an ACE","p":["The Seven Goals (aka, VAULTIS ) to become a data-centric organization, as outlined by the Department of Defense, are:","Make Data Visible: Enabling consumers of data to easily locate and search for the data they need.","Make Data Accessible: Ensuring consumers of data can retrieve and access the data they need.","Make Data Understandable: Ensuring that the context, content and applicability of the data is clear and comprehensive","Make Data Linked: Exploiting innate relationships between data elements to allow easy connections and understanding of relationships","Make Data Trustworthy: Implementing robust data quality controls to ensure data is accurate, complete, and up-to-date","Make Data Interoperable: Implementing standards for data representation and comprehension, to ensure easy sharing and use across different systems and platforms","Make Data Secure: Protecting consumer data from unauthorized use or manipulation by implementing security controls and measures such as encryption, access controls, and monitoring systems.","Achieving these goals would help an organization to become data-centric, which would allow them to make better data-driven decisions, identify opportunities for innovation, and improve overall efficiency. With the ability to monitor real-time actions and gather evidence across channels, it is important to ensure that the service meets the needs and expectations of customers and users. By achieving these seven goals, organizations will ensure that data is discoverable, accessible, usable, linked, and trustworthy.","Here are some potential impacts that an ACE may have in your environment."]}],[{"l":"Understanding the Hierarchy of Data Value","p":["When it comes to data, not all data are created equal. In fact, there are different tiers of value when it comes to data, each serving a specific purpose.","Tier 1 data is considered the most valuable, as it is used frequently and requires quick access. This type of data is typically stored on high-speed storage systems for efficient searching.","Tier 2 data is accessed less frequently, and therefore is stored on different storage systems. It may not require the same level of speed as Tier 1 data, but it is still considered important to the organization.","Tier 3 data is considered the least valuable, but it is still important for compliance purposes such as eDiscovery. This type of data is stored long-term, but is not accessed as frequently as the other tiers.","The value of data can be determined by a number of factors, including search frequency, performance, capabilities, classification, compliance, governance, and location. For example, firewall logs may be important for cybersecurity and IT operations, but not necessarily for the desktop support team.","By understanding the hierarchy of data value, organizations can better align their IT infrastructure and contextualize their data for the appropriate audience. Using the IT Service Blueprint method, organizations can map out the technology used to provide IT services to specific populations of users. This helps to ensure that the right data is being collected, stored, and accessed in the most efficient and effective way possible."]}],[{"l":"The Role of Data Analytics in Transforming Decision Making Processes","p":["What should I do right now vs. what can I delay or hold off on until the priorities are taken care of?","When you use data to navigate in a new city or check the weather in your current location, you have applied value to that insight. This is proven by you taking the action of bringing out your phone or glancing at your smart watch. When your gaze lands on the weather app icons, you have placed some trust into the weather insights and can make a decision based on current and trustworthy information.","Out of curiosity, would you wait up to 30 seconds for weather data for the trip you are taking next week? Would you wait up to a minute… or five? What if you had to wait an entire day for the insights? There is a balance between time, availability, usefulness, and accessibility when it comes to data-driven decision making. If I have to fumble through a series of clicks on my phone to get the weather forecast, I might just revert to the news on TV or a newspaper. If neither of those are available, I will just take my umbrella anyway!","There’s a common thread across every level of an organization - that is, we all want to make the best choice for the business, the customer, and the team. In some cases (Public Sector), there is a life or death scenario that must be factored into every decision when it comes to IT. If someone’s life depends on it, would you install a secondary (backup) internet connection? Did you know that there is very little (if any) regulation on how counties are to prepare the IT environment in the interest of human life. In other words, the jail’s internet connection goes down and risk goes up. What if there were a backup, secondary ISP (internet service provider) providing access so that the doors can be opened - allowing medical response personnel access to a detainee who is having a seizure.","If we are to put this much effort into our technology to prevent the loss of life, we surely need to be able to trust the data. If an ambulance does not have the correct information on the patient and how to get to them, they could increase the risk of someone losing their life. The ambulance needs to be able to not only navigate efficiently to the scene, but also navigate to the hospital while providing life-saving care.","We don’t have time to get into it, but all of this makes me think of entropy, likelihood, and possibilities. The possibilities are endless and equally exciting. The power of data analytics has transformed our day-to-day lives and will continue to do so. We are in the midst of something big. Web3, blockchain, decentralized systems, IoT, and AI are all emerging technologies that will change our world.","Endless, huh?…. So where does one begin? What is the best course of action?"]}],[{"l":"Storytelling With Data","p":["\"Stories have been used to dispossess and malign, but stories can also be used to empower and to humanize. Stories can break the dignity of a people, but stories can also repair that broken dignity.\"— Chimamanda Ngozi Adichie, The Danger of a Single Story, TED Talk","There are many wonderful people out there creating content and enablement material (practical wisdom) about storytelling with data. Brent Dykes wrote an excellent book called, Effective Data Storytelling and Cole Nussbaumer Knaflic has published multiple books; Storytelling With Data and recently, Storytelling With You.","What I will say here in this article is this: 1) get and read those books and 2) start by telling your data story. What do you measure, monitor, and track? For me, I religiously track my spend on fuel in a smartphone app that tracks gas mileage. I can look at this app’s dashboard for my vehicle and tell you exactly (I have missed a few fill ups) how much I spent on fuel in the last 182,000 miles. I can tell you how much I have spent (approximately) on fuel in the last 10 years. This app tells me a lot and has many more capabilities to track maintenance like oil changes and tires. Why do I use it? I have been tracking different things since I figured out how to use spreadsheets…. It’s a thing I do! But… through that experience, I reckon (John Dutton voice) it is helpful to share your data story with other people. This will help us all practice and find common language.","Jordan Morrow writes in his book, Be Data Literate, about the lack of a common language with regard to data. He also talks about the four levels of analytics; descriptive, diagnostic, predictive, and prescriptive and the three Cs - which I’ll cover later in Part III. When we frame things using these four levels, we can identify our common language.","The gas mileage app is providing me with descriptive analytics about spend, fuel consumption, vehicle wear and tear, and maintenance. If the mile per gallon suddenly drops, we can ask why and look for an answer. Was it because of headwinds, towing, or because I was late and driving fast. (Of course I would never speed!)","Based on these insights, I can predict how much I will spend on fuel next month or next year.","As you read in the beginning of this writing, I attempted to capture your interest and attention with my data story. As you find new ways to share how you consume and generate data, you also begin developing new ways of hearing data stories. The US Census made a lot more sense to me when I learned more about how data is collected and used. This becomes incredibly important when it comes to demographics or social profiles. As these skills develop, one might see Facebook, TikTok, and other social media apps a little differently simply based on the fact that your data is being used for their financial gain. But I digress!","Stories are powerful as you read in this section’s opening quote. Chimamanda accurately points out that stories are so powerful that they can replace governments, marginalize groups or individuals, or create division where there might not be (or not as much).","With so many stories and attempts to capture your attention, we—as people—tend to become overwhelmed. There is a lot of noise, many signals, and sometimes too many options. How do we decide?","\"A wealth of information creates a poverty of attention.\"— Herbert A. Simon, Economist and Political Scientist","As individuals, we often make tough decisions based on our values and ethics. These core beliefs guide us and help us navigate difficult situations. Similarly, in business, we rely on frameworks, standards, protocols, and processes to guide our decision-making. We may measure our progress against established baselines to ensure that we are on track.","When it comes to discussing data and insights, it is important to provide context and background information to help listeners understand and calibrate their thinking. For example, if we are discussing cybersecurity capabilities, we might use the MITRE ATT&CK framework to describe our progress in preventing lateral movement. By providing a clear and concise description of the insights being discussed, and how they align with a well-known framework or baseline, we can enable others to make informed decisions.","In today's world, it is crucial that we learn to identify and call out misinformation. Misinformation and propaganda have always been present, but with the proliferation of digital media, it is more important than ever to be able to distinguish between reliable and unreliable sources. By establishing a common language and understanding, we can have more meaningful and effective communication and discussions.","\"It is a capital mistake to theorize before one has data.\"— Sherlock Holmes A Study in Scarlet, Arthur Conan Doyle"]},{"i":"storytelling-with-data-the-importance-of-considering-the-user","l":"Storytelling with Data: The Importance of Considering the User","p":["The success of any data storytelling lies in its ability to engage and reach the emotion of the audience. It is crucial to consider who your audience is and tailor your presentation accordingly. While a technical audience with a high level of data literacy may be able to comprehend more complex language and visualizations, a non-technical audience may require simpler explanations, basic data visualizations, or a walk-through of what’s being presented. Using metaphors and analogies are very helpful tools when customizing your presentation or content.","It is also essential to consider their goals, interests, and motivations. What do they hope to learn from the data? How can the insights from the data be applied to their needs? By aligning your data storytelling with the audience's needs and interests, you can create a relevant and engaging story or presentation that effectively communicates the value of the data.","Finally, when considering the context of the data, user stories can also be used to identify key insights and actionable steps that can be taken based on those insights. This helps to make the data more valuable and applicable to your audience, and helps them understand the implications of the data."]}],[{"l":"Data at the Heart of Modern Life","p":["In this section, we will explore the 5 Vs, the growth of IoT and sensing devices, the impact of mobile and cloud computing on data generations, and the importance of data in a pandemic.","As our world becomes increasingly digitized, we are generating and collecting vast amounts of data from a variety of sources. From the sensing devices placed throughout our physical environment to the data we create through our daily interactions and activities, the amount of data we produce is vast and ever-growing.","But data is more than just a collection of numbers and information. It is a precious resource, one that will last long after the systems and technologies that generate it have come and gone. As Tim Berners-Lee said,","\"Data is a precious thing and will last longer than the systems themselves.\"— Unknown","In the future, we will see data play an even greater role in our lives as technologies like blockchain allow us to exchange value peer-to-peer using digital infrastructure. Autonomous vehicles, energy grids, entertainment, and transportation will all be transformed by the power of data. Data is a valuable resource that will shape our world for generations to come.","As more people and more things become connected, more data is generated, consumed, analyzed, stored, destroyed, and valued.","Using blockchain technology, we can exchange value for the first time - peer-to-peer - using digital infrastructure. Fun fact: the only other peer-to-peer infrastructure we have to exchange value is cash.","The future motivates me, personally. I love technology and love thinking about all the ways that we humans will interact with and experience technology. AR, VR, xVR, whatever the reality, it sounds cool and I look forward to it!"]},{"l":"The 5 Vs of Data Analytics","p":["Data plays a central role in modern life, and organizations of all types and sizes are facing challenges in managing the volume, variety, veracity, velocity, and value of the data they generate and collect. From states to universities, counties to tribes and pueblos, school districts to utilities, academic medical centers to businesses, the challenges of data management are widespread and complex. In this section, we will delve into each of the 5Vs and explore the key considerations for an effective data strategy."]},{"l":"1. Volume","p":["As organizations continue to digitize, the volume of data being generated on a daily basis has become overwhelming. In fact, according to recent estimates, the average digital organization creates terabytes of data each day. Here’s a list of some important questions to consider, as the volume of data can have a significant impact.","How much data does your digital organization create every day?","How much of that data that is created by the organization needs to be collected?","How much of the data is clean and is usable?","How much of that data that is collected needs to be analyzed or processed?","How much of that data that is collected needs to be retained?","How will the volume of data impact the organization’s governance program?","How much of the digital organization’s data is not being collected?","Volume can be derived from # of users, # of endpoints, and other factors."]},{"l":"2. Variety","p":["There is a growing movement within the cybersecurity industry to establish a common standard for sharing threat intelligence and communicating threat tactics and techniques. For example, Splunk recently announced a partnership with Amazon Web Services (AWS) to create the Splunk Security Lake, which aims to provide a centralized platform for storing, analyzing, and sharing security data. However, until there is a widely-accepted industry-wide standard for data sharing, organizations will continue to deal with a variety of data sources.","Each firewall vendor has a different log format, and endpoint telemetry may contain more information than what is reported by the firewall. While these data sources can complement each other, the differences in format can make it difficult for organizations to effectively analyze and utilize the data. Splunk provides tools that can help normalize this variety of data using a Common Information Model (CIM). By using CIM, organizations can more easily integrate and analyze data from multiple sources, enabling them to make more informed security decisions.","The following five items are key components of a successful data analytics program:"]},{"l":"Integration","p":["This refers to the process of combining data from multiple sources, such as databases, systems, and applications, in order to gain a more comprehensive understanding of the data."]},{"l":"Quality","p":["Ensuring the quality of data is critical for accurate analysis and decision-making. This includes verifying the accuracy, completeness, and consistency of the data."]},{"l":"Governance","p":["Data governance is the set of policies, procedures, and standards that govern the use, management, and protection of data within an organization. It helps to ensure that data is used ethically and responsibly."]},{"l":"Security","p":["Protecting data from unauthorized access and ensuring the confidentiality, integrity, and availability of data is essential for any organization."]},{"l":"Analysis","p":["This refers to the process of examining, transforming, and modeling data in order to discover useful insights and inform decision-making. It can involve a variety of techniques, such as statistical analysis, machine learning, and data visualization."]},{"l":"3. Veracity","p":["Ensuring the veracity of data is essential for a successful data analytics platform. This includes considering factors such as accuracy, completeness, timeliness, consistency, and integrity, as these all contribute to the overall quality of the data. Ensuring that the data is of high quality is critical for accurate analysis and informed decision-making. There are multiple points of importance when it comes to the veracity of data:"]},{"l":"Accuracy","p":["Veracity refers to the truthfulness and accuracy of data. Data that is not accurate or reliable can lead to incorrect insights and decisions, which can have serious consequences for an organization."]},{"l":"Completenes","p":["Veracity also refers to the completeness of data. Data that is incomplete or missing key elements can impact the accuracy of analytics and insights, as well as the usefulness of the data for decision-making."]},{"l":"Timeliness","p":["Veracity also refers to the timeliness of data. Data that is not up-to-date or current can be less useful for decision-making, as it may not reflect the current state of the organization or industry."]},{"l":"Consistency","p":["Veracity also refers to the consistency of data. Data that is not consistent or does not follow established standards and conventions can be difficult to use and integrate, and may lead to incorrect insights and decisions."]},{"l":"Integrity","p":["Veracity also refers to the integrity of data. Data that has been compromised or altered in any way can impact the accuracy and reliability of analytics and insights, as well as the trustworthiness of the data for decision-making. Ensuring the veracity of data is critical for the success of any data analytics and decision-making efforts."]},{"l":"4. Velocity","p":["Data velocity is a measure of how fast data is being generated and processed. It is calculated using two factors: time and amount. Speed, frequency, volume, and variety are all components of data velocity that can impact how quickly data is generated and processed.","Imagine trying to measure the flow of electricity in a circuit. Just like electricity, data can flow at different speeds and in different quantities. To accurately measure the flow of electricity, you would need to consider factors such as the amount of current (similar to volume in data), the frequency of the current (similar to frequency in data), and the type of load on the circuit (similar to variety in data). Similarly, to measure data velocity, you need to consider the speed at which data is generated, the frequency of data generation, the volume of data being generated, and the variety of data being generated.","Understanding data velocity is important because it can help organizations make informed decisions about how to process and store data. For example, if an organization is dealing with high velocity data, it may need to invest in more powerful and efficient processing and storage systems in order to keep up with the volume and speed of data being generated. On the other hand, if an organization is dealing with low velocity data, it may be able to use less expensive and less powerful systems to process and store the data. By understanding data velocity, organizations can ensure that they have the right systems and infrastructure in place to support their data analytics efforts.","Breaking down speed, frequency, volume, and variety:"]},{"l":"Speed","p":["The velocity of data refers to how quickly data is generated and processed. With the increasing amount of data being generated, it is important for organizations to have the ability to process and analyze data in real-time or near real-time to keep up with the pace of change."]},{"l":"Frequency","p":["The velocity of data also refers to the frequency at which data is generated and updated. Data that is generated and updated frequently may be more useful for decision-making, as it can provide a more current and accurate picture of the organization or industry."]},{"l":"Volume","p":["The velocity of data is also impacted by the volume of data being generated. Organizations that generate a high volume of data may need to invest in more advanced technologies and processes to handle the volume and velocity of data effectively."]},{"l":"Variety","p":["The velocity of data can also be impacted by the variety of data being generated. Data that comes from a variety of sources and formats may be more difficult to process and analyze quickly, which can impact the velocity of data."]},{"l":"5. Value","p":["What is the value of a seashell?","What is the value of a goat or a chicken?","What is the value of 1 BTC?","How about a Peso?","The value of a piece of art is determined by a number of factors.","The value of music, a concert ticket, or the autographed poster are all complex equations.","What is the value of an hour of your time?","The value of something is determined by how much it is worth to an individual or group. This value can be subjective and can vary depending on a variety of factors. For example, the value of a seashell may be different for one person compared to another, depending on the individual's personal preferences and circumstances. Similarly, the value of data and data analytics can vary depending on the context and purpose for which they are being used.","Data itself can have value, as it can be used to inform decisions and drive business outcomes. Data analytics is the process of examining, transforming, and modeling data in order to discover useful insights and inform decision-making. The value of data analytics lies in its ability to provide valuable insights and help organizations make informed decisions.","Insights are the information or understanding gained from data analytics. They can provide valuable perspective and help organizations make better decisions. The value of insights depends on their relevance, timeliness, and usefulness to the organization. By understanding the value of data, data analytics, and insights, organizations can maximize their value and achieve better outcomes."]},{"l":"The Growth of IoT and Sensing Devices","p":["The internet of things (IoT) refers to the growing network of interconnected devices, sensors, and other electronic devices that are able to collect, transmit, and exchange data. These devices are becoming increasingly prevalent in many areas of our lives, including homes, cities, transportation systems, and businesses. The rise of IoT and sensing devices is driving the proliferation of data, as these devices are able to generate, collect, and transmit vast amounts of data in real time.","The data generated by IoT and sensing devices can be used for a variety of purposes, including real-time decision making, trend analysis, and predictive modeling. The ability to collect and analyze this data in real time allows organizations to make more informed decisions and respond more quickly to changing circumstances. As the number of IoT and sensing devices continues to grow, the amount of data being generated and analyzed will also increase, leading to even greater insights and opportunities for organizations."]},{"l":"The Impact of Mobile and Cloud Computing on Data Generation","p":["Mobile and cloud computing are driving the rapid growth of data generation. The proliferation of mobile devices and the increasing use of cloud-based services have made it easier for people to access and use these technologies, leading to a significant increase in the amount of data being produced.","Analytics users expect an exceptional experience when interacting with data, and we want them to spend more time engaging with insights. This means that we need to focus on delivering a user-friendly interface and ensuring that data is easily accessible.","The pandemic has also contributed to the increase in data generation, as more people turned to online platforms for services such as grocery delivery. This trend is likely to continue as the use of mobile and cloud technologies becomes more widespread.","As the amount of data being generated continues to grow, organizations need to carefully consider how much data to collect and analyze, and how long to retain it. By identifying the systems, users, and assets that are generating the most relevant data, we can more effectively scope problems and take action to address them. Baselines and trending are also important for understanding how IT services are performing and responding to user needs."]},{"l":"The Importance of Data in a Pandemic","p":["The pandemic brought attention to the crucial role that data plays in decision making. From tracking case numbers and infection rates to determining the capacity of hospitals, data became a driving force in understanding and responding to the crisis. This emphasized the need for real-time insights and quick response times in order to keep critical services running and secure. The pandemic also highlighted the importance of defining what is essential, both in terms of services and data. Ensuring the availability of these essential services and data requires a strong and efficient IT infrastructure, which can be achieved through the use of analytics and machine-speed response."]}],[{"l":"Data-Driven Digital Organizations","p":["As technology has become increasingly important for businesses, governments, and schools, the need for secure, efficient, and effective technology solutions has grown. While technology vendors have made significant progress in providing customers with increased security, visibility, and risk reduction, many organizations still struggle to integrate and make sense of their technology and data. There are various approaches to securing and optimizing operations, but choosing the best one can be a challenge. It is important for organizations to consider factors such as speed, budget, and overall fit with their needs and goals as they evaluate their options. It's also important to remember that the journey to becoming a digital organization is not always a quick one, and may involve transitioning from traditional approaches like statistical sampling to more modern approaches like individual data point analysis.","Differences in visibility","Using real-time insights, organizations can take tailor-made actions to address specific issues and optimize their operations. By being able to ask questions, identify trends and patterns, and detect deviations from established baselines, organizations can improve their decision-making and take more targeted, effective actions. This can be especially important for leadership, stakeholders, and users such as analysts, engineers, and policy makers who rely on data and analytics to inform their work. To support these efforts, IT teams may need to reinvent themselves and provide self-service analytics tools and resources to empower users to access and analyze data on their own. Additionally, some organizations now exist entirely in the digital world and may not have physical offices, making it even more critical that they have robust analytics capabilities to support their operations."]},{"i":"how-do-you-detect-emerging-schemas-patterns-or-trends","l":":icon-question: How do you detect emerging schemas, patterns, or trends?","p":["Answer: Descriptive analytics"]}],[{"l":"Introducing Operationalizing Data Analytics Methodology","p":["ODAM (Operationalizing Data Analytics Methodology) is a methodology that helps organizations bring data analytics to the next level. Not only can it provide an efficient way to define, plan, execute, and measure data analytics initiatives, but it also can play a critical role in an organization's cybersecurity. By following this methodology, organizations can use data analytics to improve their threat detection capabilities, streamline incident investigation and make faster and more informed decisions in their incident response efforts. This can lead to more efficient and effective cybersecurity operations, help organizations stay ahead of emerging threats, and ultimately minimize the impact of security incidents on the organization and its stakeholders.","Through ODAM, you will be able to mature and operationalize its data analytics capabilities, enabling it to drive business value and support decision-making across multiple departments and use cases. By leveraging data analytics, your organization can accelerate progress and lead the way in using data to drive business success.","The key components of ODAM are outlined below."]}],[{"i":"what-problem-is-odam-trying-to-solve","l":"What problem is ODAM trying to solve?","p":["ODAM (Operationalizing Data Analytics Methodology) is a comprehensive methodology that helps organizations build a data-driven culture and achieve measurable business outcomes by providing a framework for defining, planning, executing, and measuring data analytics initiatives. It aims to solve the problem of organizations not being able to fully leverage the potential of data analytics and make data-driven decisions. Without a clear methodology and framework, organizations may struggle to align their data analytics efforts with their strategic goals and objectives, and may not be able to maximize the value of their data assets. ODAM provides a structured approach to operationalizing data analytics, enabling organizations to mature and operationalize their data analytics capabilities, and drive business value and support decision-making across multiple departments and use cases."]},{"i":"why-would-an-organization-use-odam","l":"Why would an organization use ODAM?","p":["An organization would use ODAM to mature and operationalize their data analytics capabilities to drive business value and support decision-making across multiple departments and use cases. ODAM provides a framework for defining, planning, executing, and measuring data analytics initiatives, which can help organizations align their data analytics efforts with their strategic goals and objectives, and maximize the value of their data assets. Additionally, ODAM helps organizations establish a data-driven culture and provides best practices and resources for implementing data analytics in an effective and efficient manner."]},{"i":"how-can-odam-save-your-organization-money","l":"How can ODAM save your organization money?","p":["One of the main ways is by increasing efficiency and reducing manual workflows in security operations. By automating certain processes, such as incident investigation and response, organizations can reduce the amount of time and resources required to manage security incidents. Additionally, by implementing advanced security analytics and a risk-based alerting framework, ODAM can help organizations identify and prioritize high-risk incidents more effectively, allowing them to focus their resources on the most critical threats.","Another way ODAM can save organizations money is by reducing the risk of data breaches and other security incidents. By providing a framework for defining, planning, executing, and measuring data analytics initiatives, ODAM can help organizations mature and operationalize their data analytics capabilities, which can improve their overall security posture and reduce their risk of a data breach.","ODAM can also help organizations save money by reducing the costs associated with compliance. By providing a framework for monitoring and reporting on compliance-related activities, ODAM can help organizations demonstrate compliance with regulatory requirements, which can help them avoid costly fines and penalties."]},{"i":"where-does-one-start","l":"Where does one start?!","p":["Organizations often face complex problems and challenges that they believe could be addressed through data analytics. However, they may not know where to start. To overcome these problems, ODAM can empower organizations to take the following steps.","A problem is noticed,","the problem is understood","action is taken, and","the problem is resolved."]}],[{"l":"Data Strategy Template"},{"l":"Navigation","p":["Address Common Challenges, Problems and Obstacles with ODAM","ODAM’s Impact on CDO, CIO, and CISO Goals","Focus Areas and Guiding Principles","The ODAM Use Case Lifecycle","Outcomes When Using ODAM"]},{"l":"Intro","p":["The focus should not be on a specific technique or element but rather on the broader concepts of alignment, sustainability, and resilience.","When it comes to creating a data strategy that incorporates focus on the broader concept of alignment as opposed to specific techniques, organizations can operationalize their tailored strategy for improved operational resilience.","Let’s take a look at the high-level steps that organizations can take to operationalize their customized data strategy. In Part III, I will outline the essential elements and steps to set up an ACE. An ACE would deploy and operate the data strategy.","First, the organization must develop a clear plan of action. One of the first steps in operationalizing a proactive data strategy is to develop a clear and detailed plan of action that outlines the specific steps and tasks that need to be completed to implement the strategy. This includes identifying the resources and personnel needed, establishing timelines and milestones, and identifying any potential barriers or challenges that may need to be overcome. ODAM provides a template Program Charter for you to customize and tailor to your organization.","Next, it is important to clearly define the roles and responsibilities of each team member or department within the organization to ensure that everyone knows their part in implementing the cybersecurity strategy. This means establishing a clear chain of command and delegating tasks and responsibilities to appropriate team members.","When it comes to measuring the effectiveness of the cybersecurity strategy, it is important to establish metrics and benchmarks that can be used to track progress and identify areas for improvement. This may include metrics such as the number of successful cyber attacks, the time it takes to detect and respond to threats, and the overall cost of cybersecurity efforts. ODAM will assist organizations with the prioritization of metrics and KPIs by leveraging the IT Service Blueprint methodology.","Also, be sure to regularly review and update the strategy. As the threat landscape and the needs of the organization change, it is important to regularly review and update the cybersecurity strategy to ensure that it remains effective. Organizations can conduct periodic risk assessments, integrate new technologies or controls, and update policies and procedures as needed.","Operationalizing a tailored cybersecurity strategy blueprint requires clear planning, clear roles and responsibilities, the establishment of metrics and benchmarks, and regular review and updates to ensure that the strategy remains effective in addressing the evolving threat landscape and the needs of the organization."]}],[{"i":"address-common-challenges-problems-and-obstacles-with-odam","l":"Address Common Challenges, Problems and Obstacles with ODAM","p":["Organizations can use ODAM as a fast-track in identifying and understanding common challenges, problems, and obstacles in a number of ways.","ODAM can be a powerful tool for organizations looking to identify, understand, and address challenges, problems, and obstacles. By using ODAM, organizations are able to use data analytics and advanced techniques such as machine learning to gain valuable insights into the root causes of issues or in the pursuit of developing and implementing effective solutions to address them."]}],[{"i":"odam-s-impact-on-cdo-cio-and-ciso-goals","l":"ODAM’s Impact on CDO, CIO, and CISO Goals","p":["As a CxO, there are common and shared goals.","One key way to achieve these goals is through the adoption of a comprehensive data strategy, such as ODAM. ODAM allows CIOs, CDOs, and CISOs to efficiently and effectively manage their organization's data strategy, ensuring that data is collected, processed, and used in a consistent and secure manner; which ultimately creates value. By adopting ODAM, CxOs can drive business performance, improve compliance, and promote a data-driven culture within their organization. As data becomes increasingly central to modern business operations, the role of CxOs in managing and leveraging data will only become more important."]}],[{"l":"Focus Areas and Guiding Principles"},{"l":"Asset-centric approach","p":["An asset-centric approach focuses on protecting the hardware and software assets of an organization. This may involve identifying and prioritizing the most critical assets, implementing controls and technologies to protect them, and monitoring for and responding to threats that may compromise those assets."]},{"l":"Data-centric approach","p":["A data-centric approach focuses on protecting the data of an organization. This may involve implementing controls and technologies to ensure the confidentiality, integrity, and availability of data, as well as implementing policies and procedures to govern the handling and use of data."]},{"l":"Identity-centric approach","p":["An identity-centric approach focuses on protecting the identities of individuals within an organization. This may involve implementing strong authentication and access controls, as well as implementing identity and access management (IAM) systems and processes to govern the use of identities."]},{"l":"Technology-centric approach","p":["A technology-centric approach focuses on implementing and maintaining the technologies and systems needed to protect data and ensure uptime. This may involve selecting and implementing appropriate technologies, as well as developing and implementing maintenance and support processes to ensure the ongoing health and performance of those systems."]},{"l":"Attacker-centric approach","p":["An attacker-centric approach focuses on understanding and defending against the tactics, techniques, and procedures (TTPs) used by attackers. This may involve implementing controls and technologies designed to detect and prevent attacks, as well as developing and implementing incident response plans to address attacks that do occur."]},{"l":"Risk-centric approach","p":["A risk-centric approach focuses on identifying and mitigating the risks facing an organization. This may involve conducting risk assessments to identify potential threats and vulnerabilities, and implementing controls and technologies to address those risks."]},{"l":"IT service-centric approach","p":["An IT service-centric approach focuses on ensuring the availability and performance of the IT services needed to support the business. This may involve implementing monitoring and performance management tools, as well as developing and implementing processes to ensure the availability and reliability of those services."]},{"l":"Capabilities-centric approach","p":["A capabilities-centric approach focuses on identifying and implementing the capabilities needed to protect data, keep systems up, and ensure maximum uptime. This may involve identifying the specific capabilities that are needed to support the business, and implementing technologies and processes to deliver those capabilities."]},{"l":"Use cases-centric approach","p":["A use cases-centric approach focuses on identifying and addressing the specific use cases that are most important to the business. This may involve identifying the specific needs and requirements of different business units or stakeholders, and implementing technologies and processes to support those use cases."]},{"l":"Compliance-centric approach","p":["A compliance-centric approach focuses on ensuring that the organization is compliant with relevant laws, regulations, and industry standards. This may involve implementing controls and processes to meet specific compliance requirements, as well as establishing policies and procedures to ensure ongoing compliance."]},{"l":"Remediation-centric approach","p":["A remediation-centric approach focuses on identifying and addressing issues or problems as they arise. This may involve implementing monitoring and alerting systems to detect issues, as well as developing and implementing processes to quickly resolve those issues."]},{"l":"Roadmap-centric approach","p":["A roadmap-centric approach focuses on establishing a long-term plan for protecting data, keeping systems up, and ensuring maximum uptime. This may involve developing a roadmap that outlines the specific steps and actions needed to achieve these goals, as well as establishing timelines and milestones to track progress."]},{"l":"Product-centric approach","p":["A product-centric approach focuses on implementing specific products or technologies to support the organization's efforts to protect data, keep systems up, and ensure maximum uptime. This may involve selecting and implementing appropriate products or technologies, as well as developing and implementing processes to maintain and support those products/"]},{"l":"Time-centric approach","p":["A time-centric approach to data, cybersecurity, and analytics would focus on the urgency and critical timelines that need to be met, such as deadlines for audits or requirements from cybersecurity insurance companies. This approach would prioritize and address the most pressing concerns and needs in a timely manner."]},{"l":"Zero-trust centric approach","p":["A zero-trust centric approach is based on the premise that organizations should not trust any user, device, or network by default, and that all access should be authenticated and authorized before being granted. This approach may involve implementing technologies such as multi-factor authentication, network segmentation, and micro-segmentation to secure access to systems and data."]},{"l":"Best approach","p":["The best approach for your organization will depend on your specific needs and goals. It may be helpful to consider a combination of these approaches, or to customize an approach that is tailored to the unique needs of your organization."]}],[{"l":"The ODAM Use Case Lifecycle","p":["A Guide to Developing and Implementing Data Analytics","The ODAM (Operationalizing Data Analytics Methodology) use case lifecycle is a comprehensive process for identifying and addressing data-driven problems and opportunities. It involves twelve steps, including defining requirements, identifying data sources, designing logic, onboarding and validating data, developing a proof of concept, performing user acceptance testing, documenting operational procedures, providing training and enablement, promoting to production, and optimizing and measuring performance. Each step is designed to help organizations develop and implement effective data analytics solutions that meet the needs and expectations of their users. By following the ODAM use case lifecycle, organizations can ensure that their data analytics projects are well-planned, well-executed, and deliver value to their stakeholders.","Here is a brief explanation of each of the top-level items in a use case life cycle strategy. Each section represents a step in the process for developing a use case for data analytics, from defining the requirements and identifying data sources, to developing and testing the use case to ensure that it meets the needs and expectations of the users."]}],[{"l":"Outcomes When Using ODAM"},{"l":"The Power of Visibility in an IT Environment","p":["In today's digital world, visibility is key to the success of any organization. In the realm of IT, visibility refers to the ability to see and understand what is happening within your systems and networks. By gaining visibility into your IT environment, you can take control and make informed decisions that drive your business forward.","Here are some specific benefits of visibility in an IT environment.","Visibility in the IT environment is essential for driving success and growth in any organization. By leveraging tools and technologies that provide visibility into your systems and networks, you can take control and make data-driven decisions that move your business forward."]}],[{"l":"Using ODAM to Set up your ACE"},{"l":"Navigation","p":["Executive Sponsorship","Building a Data-Driven Culture","ODAM for Setting Up a Successful Analytics Center of Excellence","Expected Outcomes and Goals for an ACE","Developing a Data Analytics Roadmap","The Data-Driven Product Development Process"]},{"l":"Into","p":["An Analytics Center of Excellence (ACE) is a dedicated team or function within an organization that is responsible for promoting and enabling the use of data analytics to support business objectives. The goal of an ACE is to provide a centralized resource for data analytics expertise, tools, and processes that can be leveraged across the organization to drive data-driven decision making and innovation. Some key responsibilities of an ACE include:","Defining and enforcing data governance policies and standards.","Identifying and prioritizing data analytics use cases.","Providing guidance and support to business units in the development and execution of data analytics projects.","Developing and maintaining a data analytics roadmap and strategy.","Providing training and enablement to data analysts and other team members.","Evaluating and selecting data analytics tools and technologies.","The goal of an ACE is to help organizations derive value from their data assets by providing a centralized resource for data analytics expertise, tools, and processes."]}],[{"l":"Executive Sponsorship","p":["Full executive sponsorship is critical for the success of an Analytics Center of Excellence (ACE).","Here are the four main reasons:","Executive sponsorship is critical for the success of an Analytics ACE because it helps to ensure that the ACE has the buy-in and support, alignment with business objectives, governance and oversight, and visibility and influence needed to deliver value to the organization."]}],[{"l":"Building a Data-Driven Culture","p":["The Role of Communication, Engagement, and Collaboration","Effective data analytics culture in an organization is built on a foundation of strong communication, engagement, and collaboration. One key element of this is regular office hours or \"ask me anything\" sessions, in which team members can ask questions, seek guidance, and share insights with one another. Outreach to the broader organization, through events, community building initiatives, and other forms of engagement, can also be important for fostering a data-driven culture. Establishing a feedback loop, through which team members can share their experiences and ideas, is another key component of building a successful data analytics culture. Finally, social proof - the idea that people are more likely to adopt a certain behavior or attitude if they see others doing the same - can be a powerful force in driving the adoption of data analytics practices and tools within an organization. By focusing on these key elements, organizations can build a culture that is conducive to the effective use of data analytics to drive business decisions and outcomes."]}],[{"l":"ODAM for Setting Up a Successful Analytics Center of Excellence","p":["Key High-Level Activities","When it comes to using ODAM to set up your analytics center of excellence (ACE), there are key activities that are critical to the organization’s overall success. These include program management, platform management, getting data in, creating analytics, receiving feedback, education and training, the identification of IT services, and IT service visualization.","Program management involves overseeing and coordinating the various activities and initiatives that are part of the ACE. This includes setting goals and objectives, establishing policies and procedures, and ensuring that the ACE is aligned with the overall goals and objectives of the organization.","Education and training are important components of the ACE, as they ensure that team members have the necessary skills and knowledge to effectively use and contribute to the ACE. This may include training on specific technologies or processes, or providing guidance and support to help team members become proficient in using the ACE.","Platform management involves the deployment and maintenance of the technical infrastructure and tools that are needed to support the ACE. This may include data storage, data processing, data visualization, and other technical components.","Getting data in involves the process of acquiring and preparing data for analysis. This may include extracting data from various sources, cleansing the data to ensure its quality and integrity, and transforming the data into a usable format.","Creating analytics involves the development of algorithms and models that can be used to analyze and interpret the data. This may include statistical analysis, machine learning, or other techniques.","The identification of IT services involves the process of mapping out the various IT services that are used by the organization and how they are related to one another. This helps to provide context and understanding of the data and analytics being generated by the ACE.","Receiving feedback involves gathering and incorporating feedback from users and stakeholders on the performance and effectiveness of the ACE. This may include gathering feedback on the usefulness of the analytics, the accuracy of the results, and the user experience."]},{"l":"Understanding the IT Service Blueprint","p":["An IT Service Blueprint is a visual representation of the various components and processes that make up an IT service. It typically includes information about the service's stakeholders, customer needs, the service's value proposition, and the various components (such as processes, technology, and people) that are required to deliver the service. The purpose of an IT Service Blueprint is to provide a clear and comprehensive overview of an IT service, which can help organizations understand how the service is delivered and identify areas for improvement. ODAM provides a framework for defining, planning, executing, and measuring data analytics initiatives, which can be used to create an IT Service Blueprint for an IT service that leverages data analytics to deliver value to stakeholders, end users, and customers.","The IT Service Blueprint is a crucial tool for IT professionals looking to understand and manage the various components and processes involved in delivering an IT service. By providing a clear and comprehensive overview of the service, the IT Service Blueprint helps to break down silos and reveal opportunities for improvement and optimization.","One of the key benefits of the IT Service Blueprint is its ability to provide a common \"big picture\" view of the service, which can be helpful in understanding the service in a holistic manner. This view takes into account the situational and contextual factors that may impact the service, and helps to provide a point of view that is focused on the approach to delivering the service.","In addition to considering the current state of the service, the IT Service Blueprint also looks at the future state, and can help to identify any potential gaps or areas for improvement. With the ability to monitor real-time actions and gather evidence across channels, the IT Service Blueprint can be a valuable tool for ensuring that the service meets the needs and expectations of customers and users."]},{"l":"An IT Service Blueprint typically includes a number of different elements","p":["An IT Service Blueprint is a visual representation of the systems, processes, and resources that are required to deliver a specific IT service. It helps IT professionals understand how the various components of the service fit together and interact, and ensures that the service meets the needs and expectations of customers and users."]},{"l":"Service components","p":["These are the specific components or elements that make up the IT service, such as hardware, software, processes, or people."]},{"l":"Service dependencies","p":["These are the relationships between different service components, and how they depend on one another to function properly."]},{"l":"Service interactions","p":["These are the ways in which the service interacts with other systems, processes, or resources within the organization."]},{"l":"Service delivery channels","p":["These are the ways in which the service is delivered to customers or users, such as through self-service portals, phone support, or on-site support."]},{"l":"Service metrics","p":["These are the measures or metrics used to track and assess the performance of the service, such as availability, reliability, or customer satisfaction."]},{"l":"Alignment map","p":["An alignment map that shows how individuals interact with a digital organization is a visual representation of the ways in which people interact with and engage with the various digital systems, processes, and resources that are available within the organization. The purpose of such an alignment map is to provide a clear and comprehensive overview of how people use and access digital resources, and to help identify any gaps or areas for improvement in the user experience. An alignment map might include a number of different elements.","User profiles These are the different types of users or customer segments that interact with the digital organization, and may include categories such as employees, customers, partners, or stakeholders.","User journeys These are the steps or actions that users take when interacting with the digital organization, such as visiting a website, filling out a form, or making a purchase.","User touchpoints These are the specific points of interaction between users and the digital organization, such as a website, mobile app, or customer service hotline.","User feedback This is the feedback and insights that users provide about their experience with the digital organization, including their needs, preferences, and areas for improvement.","An alignment map that shows how individuals interact with a digital organization can be a valuable tool for understanding the user experience and identifying areas for improvement. It can help to identify any gaps or problems in the user journey, and to develop strategies for improving the user experience and engagement with the digital organization."]},{"l":"Benefits to using an IT Service Blueprint","p":["Lastly, infusing your supportive presence into the data is crucial for achieving success with your data analytics efforts. By leveraging the principles of ODAM, you can build a strong Analytics Center of Excellence that is well-equipped to support your business goals and objectives. By infusing observability into each IT Service Blueprint, you can help to ensure that your data analytics initiatives are aligned with your organization's values and are driving meaningful impact.","\"The greatest value of a picture is when it forces us to notice what we never expected to see.\"— John W. Tukey, Mathematician"]},{"l":"Breaks down silos","p":["An IT Service Blueprint helps to break down silos by providing a common view of the service, and how the various components and processes fit together. This can help to improve collaboration and communication within the IT team, and can lead to more efficient and effective service delivery."]},{"l":"Reveals opportunity","p":["By providing a comprehensive view of the service, an IT Service Blueprint can help IT professionals identify opportunities for improvement, such as streamlining processes or optimizing resources."]},{"i":"common-big-picture","l":"Common \"big picture\"","p":["An IT Service Blueprint helps to provide a common \"big picture\" view of the service, which can help to improve understanding and alignment among team members and stakeholders."]},{"l":"Holistic","p":["An IT Service Blueprint takes a holistic view of the service, and helps IT professionals understand how the various components and processes fit together as a whole."]},{"i":"situational-contextual","l":"Situational/contextual","p":["An IT Service Blueprint provides a situational and contextual view of the service, and helps IT professionals understand how the service operates in different situations and contexts."]},{"l":"Point of view","p":["An IT Service Blueprint provides a point of view on the service, and helps IT professionals understand how the service is perceived by different stakeholders."]},{"i":"there-s-a-scope","l":"There's a scope","p":["An IT Service Blueprint defines the scope of the service, and helps IT professionals understand what is and is not included in the service."]},{"l":"Focus on the approach","p":["An IT Service Blueprint focuses on the approach to delivering the service, and helps IT professionals understand how to best deliver the service to meet the needs and expectations of customers and users."]},{"l":"Consider both current and future states","p":["An IT Service Blueprint considers both the current and future states of the service, and helps IT professionals plan for the future evolution of the service."]},{"l":"Real-time actions monitoring","p":["An IT Service Blueprint helps IT professionals monitor the service in real-time, and take timely action to address any issues or problems that may arise."]},{"i":"evidence-data-across-channels","l":"Evidence (data) across channels","p":["An IT Service Blueprint provides evidence (data) across channels, and helps IT professionals understand how the service is being used and perceived by different stakeholders."]}],[{"l":"Expected Outcomes and Goals for an ACE","p":["\"Our dilemma is that we hate change and love it at the same time; what we really want is for things to remain the same but get better.\"— Sydney J. Harris, Journalist and Author","It's natural to have mixed feelings about change, as it can bring both opportunities and challenges. However, the desire to improve and make things better is a common and important goal for many organizations.","There are many things organizations and individuals can do to make things better. The question is: what motivates organizations and you to make change, challenge the status quo, and take action? For me and many others, there is plenty of motivation when I think of what the impact of the solution will be.","The impact of a data analytics program is likely to depend on a number of factors, including the quality and relevance of the data, the design and layout of the dashboard or report, and the intended audience and purpose."]},{"l":"Setting Your Intention and Achieving Success","p":["Define clear objectives and goals: It is important to define clear objectives and goals for the data analytics program, so that the team knows what they are working towards and can measure their progress. This may involve setting specific targets or milestones, such as reducing the mean time to detect (MTTD) or mean time to respond (MTTR) to threats, or improving the accuracy of threat detections.","Develop a clear strategy and roadmap: It is important to develop a clear strategy and roadmap for the data analytics program, so that the team knows what steps need to be taken to achieve their objectives and goals. This may involve identifying the specific processes and procedures that will be followed, as well as the technologies and tools that will be used.","Foster a culture of continuous improvement: To achieve success in the long term, it is important to foster a culture of continuous improvement within the data analytics program. This may involve regularly reviewing and analyzing the program's performance, identifying areas for improvement, and implementing changes as needed to ensure that the program is meeting its objectives and goals.","Identify the data sources and tools needed: In order to achieve success in the areas listed, it is important to identify the data sources and tools that will be needed to support the program. This may involve evaluating and selecting data analytics platforms, such as Splunk, as well as identifying the specific data sources that will be used to support the program.","Maturing the SOC","Monitoring Services/Critical Applications/Network Monitoring","More Visibility into Endpoints","Offering Splunk ‘as-a-service’ to subordinate agencies or entities (e.g. multiple agencies within a State)","Offering Splunk as a Service","Reduce Risk","Reducing MTTI/MTTR","Replace Legacy tools","Replacing Legacy Tools","Set up a SOC","These recommendations can help people who are responsible for an organization's data analytics program to set their intention and achieve success in the areas of data collection and correlation, detection, monitoring, threat hunting, triage, investigations, and incident response.","What successful outcomes look like:"]},{"l":"Understanding Analytics Maturity","p":["Begin With Evaluating Your Organization's Capabilities","Analytics maturity refers to the level of capability, expertise, and sophistication that an organization has in using data analytics to support its mission and objectives. The structure of analytics maturity is typically understood in terms of a hierarchy or progression of increasing capabilities, with each level building on the insights and capabilities of the previous level.","There are many different frameworks and models that have been developed to understand and measure analytics maturity, and the specific categories and definitions may vary depending on the context and perspective. Here are a few common approaches to understanding analytics maturity.","These models typically define a set of stages or levels of analytics maturity, and may include categories such as \"ad hoc\" or \"reactive,\" \"repeatable,\" \"strategic,\" and \"advanced.\" These models typically describe the characteristics and capabilities of organizations at each stage of analytics maturity, and provide guidance on how to progress from one stage to the next.","These frameworks typically define a set of domains or areas of focus that are important for analytics maturity, and may include categories such as data governance, data management, data analytics, and data visualization. These frameworks typically describe the key practices and capabilities that organizations need to develop in each domain in order to achieve analytics maturity.","These assessments are designed to help organizations understand their current level of analytics maturity, and to identify areas for improvement. These assessments may involve self-assessment tools, interviews with key stakeholders, or other methods of gathering data and insights.","Analytics maturity is an important concept that helps organizations to understand their current capabilities and strengths in using data analytics, and to identify areas for improvement and development."]},{"l":"Assessing Your Current Data Analytics Maturity Level","p":["By utilizing ODAM, organizations can efficiently and effectively move through the five levels of data analytics maturity, and ultimately reach the optimized stage where they are consistently achieving measurable business outcomes through their data analytics efforts.","The five levels of data analytics maturity within an organization."]},{"i":"level-1-initial","l":"Level 1: Initial","p":["At this stage, the organization is just starting to explore data analytics and has little to no infrastructure or processes in place. The organization is primarily focused on identifying potential use cases and building basic understanding of data analytics concepts."]},{"i":"level-2-basic","l":"Level 2: Basic","p":["At this stage, the organization has established some basic infrastructure and processes for data analytics, but is still in the early stages of implementation. The organization is primarily focused on building foundational capabilities such as data governance and data integration."]},{"i":"level-3-intermediate","l":"Level 3: Intermediate","p":["At this stage, the organization has established a mature data analytics infrastructure and has implemented a range of data analytics use cases. The organization is focused on expanding the use of data analytics across the organization and building advanced capabilities such as machine learning and predictive analytics."]},{"i":"level-4-advanced","l":"Level 4: Advanced","p":["At this stage, the organization has fully integrated data analytics into its business operations and has a mature data analytics culture. The organization is focused on driving business value and continuous improvement through advanced data analytics techniques and technologies."]},{"i":"level-5-optimized","l":"Level 5: Optimized","p":["At this stage, the organization has fully optimized its data analytics operations and is consistently achieving measurable business outcomes. The organization is focused on driving innovation and exploring new technologies to stay ahead of the curve."]},{"i":"what-are-some-discovery-questions-one-could-ask-to-determine-the-level-of-data-analytics-maturity-in-an-organization","l":":icon-question: What are some discovery questions one could ask to determine the level of data analytics maturity in an organization?","p":["What are the primary business problems or goals that you are trying to solve with data analytics?","What are the main challenges or pain points that your organization is currently facing in terms of data analytics?","How is data currently being used across different departments and business functions within your organization?","What types of data are you currently collecting, storing, and analyzing?","How is data quality and governance being managed within your organization?","What types of data analytics tools and technologies are currently being used within your organization?","How is data analytics currently being integrated into your organization's decision-making processes?","How is data analytics currently being used for strategic planning and forecasting within your organization?","What is the current level of data literacy and skills within your organization?","What are the current performance metrics and KPIs that are being used to measure the success of your data analytics initiatives?","It is important to ask these discovery questions and take a self-assessment of an organization's data analytics maturity because it provides a clear understanding of the organization's current capabilities, strengths, and areas for improvement. By understanding where an organization currently stands in terms of data analytics maturity, you can begin to identify and prioritize the areas that will have the greatest impact on their business outcomes. Furthermore, it allows for a realistic and actionable strategy to be developed for the organization to evolve their data analytics capabilities."]},{"i":"take-the-data-analytics-maturity-self-assessment-today","l":"Take the Data Analytics Maturity Self-Assessment today!","p":["Self-Assessment","The ODAM data analytics maturity self-assessment tool is a powerful resource for organizations looking to evaluate their current level of data analytics maturity. The assessment includes 50 questions, with 10 questions dedicated to each of the five levels of data analytics maturity. The self-assessment is an important step in the ODAM process as it helps organizations identify their current strengths and weaknesses, and provides a clear roadmap for how to improve their data analytics capabilities. By taking the self-assessment, organizations will be able to understand where they stand in terms of data analytics maturity and how to move forward with a strategic plan. The self-assessment tool is an easy way to start the journey of operationalizing data analytics, and it is a great starting point for any organization looking to take advantage of the power of data analytics."]},{"l":"Self-Service Analytics","p":["Self-service data analytics is a type of data analytics approach in which users are able to access, analyze, and visualize data without the need for specialized technical skills or assistance from IT or data analysts. In a self-service data analytics environment, users typically have access to data analytics tools and platforms that are easy to use and allow them to explore data and create reports and visualizations without the need for coding or other technical expertise.","Self-service data analytics is designed to enable organizations to more quickly and easily gain insights from their data, and to enable users to more easily access and analyze data to support their decision-making and problem-solving. It is often used in conjunction with other types of data analytics approaches, such as centralized data analytics or business intelligence, to provide a more flexible and scalable approach to data analytics.","Self-service data analytics is designed to enable organizations to more easily and effectively leverage their data assets to support their mission and objectives."]},{"l":"Maximizing Value and Minimizing Risk","p":["How ODAM Can Help Your Organization","ODAM helps organizations reduce risk by providing a clear set of steps and guidelines for planning, implementing, and managing data analytics projects. This structured approach helps to ensure that projects are well-defined and that all necessary steps are taken to ensure their success.","ODAM also helps organizations reduce risk by providing guidance on how to identify and prioritize the most important business needs and objectives, and how to align data analytics efforts with those needs. This helps organizations to focus on the most important data analytics projects and avoid wasting resources on less important or less valuable projects.","Finally, ODAM assists organizations in reducing risk by guiding them through the process of identifying and managing potential risks and challenges that may arise during the course of a data analytics project. Identifying potential sources of risk, developing contingency plans, and monitoring and managing risk throughout the project are all part of this. Organizations can reduce risk and increase the likelihood of success for their data analytics projects by adhering to the ODAM principles."]},{"l":"Risk as a Factor","p":["Gathering a basic inventory - lays the foundation for building a resilient operation within your organization.","How can we simplify things so that everyone with the organization can have an ample understanding of cybersecurity but more specifically, an understanding of how priorities are established. I’m a big believer in the “show your work” phrase we learned in elementary school when practicing math. Organizations should be able to quickly and easily show what pieces of technology and process are relevant to their mission and what data was used to derive findings.","How would they come after the asset?","Measuring and Managing Information Risk; A FAIR Approach","Over the last decade I’ve observed many ways and attempts of describing risk, priority, and worst case scenarios. In other words, there are many ways of describing risk, priorities, and what ifs. The cybersecurity field has many approaches and offerings that try to solve this problem (i.e., lack of a common language) but are still falling short.","Risk assessment should be based on objective and quantifiable data, rather than subjective opinions or assumptions.","Risk is a function of the likelihood of an event occurring and the impact of that event.","Risk management should be an ongoing process, rather than a one-time event.","Risk management should be based on a consistent and repeatable process, using a common language and set of principles.","Risk should be measured in terms of loss exposure, which is the potential financial impact of a risk event.","The FAIR (factor analysis of information risk) approach is a framework for understanding, analyzing, and managing cyber risk. It is based on the idea that risk should be understood and managed in a consistent and objective way, using a common language and set of principles.","The FAIR approach is based on five key principles:","The FAIR approach provides a framework for understanding and managing cyber risk in a consistent and objective way, using a common language and set of principles. This can help organizations to better understand their risks and make informed decisions about how to manage those risks.","The word impact means different things to different people. For instance, negative impacts to private businesses are different from those of the impacts to county governments. If a ransomware event happens in a business, the impact could be lost revenue and lost productivity. If a ransomware event happens in a county, terrible, life-threatening circumstances can arise. County’s usually operate jails and other essential services like (e)911. If a ransomware attack prevents doors from opening or phone lines to ring, people could lose their lives.","To that end, I believe that organizations that take a methodical approach to identifying IT infrastructure (hardware and software) and processes relevant to the mission are more successful in keeping things secure, available, and free from issues. This first step - gathering a basic inventory - lays the foundation for building a resilient operation within your organization.","What is the scope of the risk scenario and what is the risk statement?","What’s the asset/thing you’re trying to protect?","What’s the impact if the attacker is successful?","Who is motivated to attack? Who might come after the asset? (Threat actors)."]},{"i":"it-governance-alignment","l":"\"IT Governance Alignment","p":["When you say that your goal is to align with IT governance, it means that you want to ensure that the analytics center of excellence (ACE) aligns with IT governance policies, processes, and practices of the organization. IT governance refers to the overall framework and approach that an organization uses to manage and control its IT assets and resources, and to ensure that they support and align with the objectives and goals of the organization.","Aligning the ACE with IT governance can help to ensure that the ACE is able to effectively support the mission and objectives of the organization, and that it is able to operate in a way that is consistent with the organization's IT policies and practices. This may involve ensuring that the ACE adheres to the same security, privacy, and compliance standards as other IT assets and resources, and that it is integrated into the organization's IT infrastructure and processes.","Aligning the ACE with IT governance can help to ensure that the ACE is able to effectively support the needs of the organization, and that it is able to operate in a way that is consistent with the organization's IT policies and practices."]},{"l":"Data Governance Alignment","p":["Aligning the ACE with data governance can help to ensure that the ACE is able to effectively support the mission and objectives of the organization, and that it is able to operate in a way that is consistent with the organization's data policies and practices. This may involve ensuring that the ACE adheres to the same data quality, security, privacy, and compliance standards as other data assets and resources, and that it is integrated into the organization's data infrastructure and processes.","Aligning the ACE with data governance can help to ensure that the ACE is able to effectively support the needs of the organization, and that it is able to operate in a way that is consistent with the organization's data policies and practices."]},{"l":"Business Strategy Alignment","p":["Aligning the ACE with the business strategy can help to ensure that it is able to effectively support the mission and objectives of the organization, and that it is able to contribute to the organization's success. This may involve ensuring that the ACE is focused on the right priorities and goals, and that it is able to provide value and insights that support the organization's business strategy.","Aligning the ACE with the business strategy can help to ensure that you are able to effectively support the needs of your organization, and that it is able to contribute to the organization's success."]},{"l":"Communication of Services Offerings and Capabilities","p":["Here are some recommendations for the best way to communicate and describe data analytics services or capabilities:","Clearly define the problem or opportunity that the data analytics service or capability is designed to address: Start by clearly defining the problem or opportunity that the data analytics service or capability is designed to address. This will help to provide context and relevance for the service or capability, and will make it easier for users to understand how it can be applied.","Describe the key features and capabilities of the service or capability: Next, describe the key features and capabilities of the service or capability. This may include the types of data that it can analyze, the types of insights or outputs that it can provide, and any other key features or functionality that make it unique or valuable.","Explain how the service or capability can be used: Clearly explain how the service or capability can be used, including any specific steps or processes involved. This will help users to understand how they can leverage the service or capability to address their needs or goals.","Provide examples of how the service or capability has been used successfully: Provide examples of how the service or capability has been used successfully in the past. This will help users to better understand the potential value and applications of the service or capability, and will help to build confidence and trust in its capabilities.","Clearly communicate any requirements or limitations of the service or capability: Clearly communicate any requirements or limitations of the service or capability. This will help users to understand any constraints or limitations that they may need to consider when using the service or capability, and will help to manage expectations."]},{"l":"Evangelism of Data Literacy and Data Analytics","p":["The phrase \"evangelism of data analytics and data literacy\" refers to the act of promoting and advocating for the importance and value of data analytics and data literacy within an organization. This includes educating employees and stakeholders about the benefits and applications of data analytics, and helping to build awareness and understanding of data literacy concepts and skills.","Data analytics refers to the process of collecting, organizing, and analyzing data to generate insights and inform decision-making. Data literacy refers to the ability to understand and use data effectively, including the ability to read, work with, analyze, and communicate data.","Evangelism of data literacy and data analytics includes providing training and education on data analytics and data literacy concepts, promoting the use of data analytics and data literacy tools and techniques, and advocating for the importance of data analytics and data literacy within the organization.","The goal of evangelism of data literacy and data analytics is to help organizations to more effectively leverage their data assets to support their mission and objectives, and to help employees and stakeholders to develop the skills and knowledge they need to use data effectively."]},{"l":"The 3 Cs of Data Literacy","p":["Jordan Morrow, a data literacy expert, identified the three Cs of data literacy: curiosity, creativity, and critical thinking. These three traits are essential for individuals to have in order to effectively work with and understand data. Morrow believes that by fostering these qualities in individuals, they are better equipped to approach data with a curious and open mind, generate creative insights, and think critically about the data they are working with.","Creativity is the ability to generate original and innovative ideas, and it is an important aspect of data literacy because it allows individuals to approach data with a fresh perspective and to find new and creative ways to analyze and interpret it. By being creative with data, individuals can be more likely to uncover new insights and identify opportunities that may not be immediately apparent. Creativity can also help individuals to see data in a different light, enabling them to discover new patterns and trends and make connections that might not have been previously considered. Overall, creativity is an essential component of data literacy that helps individuals to think creatively, explore new ideas, and find novel solutions to complex data problems.","Curiosity is a key aspect of data literacy. It drives individuals to seek out new information and to explore different data sources and perspectives. Being curious about data can help individuals to ask the right questions and to look for patterns and trends that might not be immediately obvious. This, in turn, can lead to new insights and a deeper understanding of the data.","Critical thinking is the ability to analyze and evaluate data in a logical and systematic manner, including considering the sources, quality, and implications of the data. This skill is essential for making informed and sound decisions based on data."]},{"l":"The Four Levels of Data Analytics","p":["Jordan Morrow also defined the four levels of analytics as descriptive, diagnostic, predictive, and prescriptive. These levels describe the increasing complexity and sophistication of data analysis, from simply describing what has happened in the past, to predicting future outcomes and prescribing actions based on those predictions.","These four levels of data analytics represent a hierarchy of increasing complexity and sophistication, with each level building on the insights and capabilities of the previous level.","Exploratory data analytics typically refers to the process of examining data in order to discover patterns, trends, and insights that might not be immediately apparent. This process may involve using various techniques and tools to visualize and analyze data, and to identify relationships and correlations within the data.","Exploratory data analytics is an important aspect of data analysis, as it can help organizations to better understand their data, to identify trends and patterns that might not be immediately apparent, and to generate insights and ideas that can inform decision-making and problem-solving.","In terms of the levels of data analytics that I described earlier, exploratory data analytics could be considered to fall under the category of diagnostic analytics. This approach to data analytics often involves investigating data to identify patterns and trends, and advanced techniques. For example, identifying correlations and causes or using data mining techniques to uncover hidden insights."]},{"i":"descriptive-analytics-level-1","l":"Descriptive analytics (Level 1)","p":["This level of data analytics involves summarizing and describing data, and may include activities such as generating reports, calculating totals and averages, and creating charts and graphs."]},{"i":"diagnostic-analytics-level-2","l":"Diagnostic analytics (Level 2)","p":["This level of data analytics involves investigating data to identify patterns and trends, and may include activities such as identifying correlations and causes, and using data mining techniques to uncover hidden insights."]},{"i":"predictive-analytics-level-3","l":"Predictive analytics (Level 3)","p":["This level of data analytics involves using data and statistical models to make predictions about future outcomes or events, and may include activities such as building predictive models, forecasting future trends, and simulating scenarios."]},{"i":"prescriptive-analytics-level-4","l":"Prescriptive analytics (Level 4)","p":["This level of data analytics involves using data and analytical insights to recommend specific actions or decisions, and may include activities such as optimization, simulation, and decision-making support."]},{"l":"Data Analytics Support and Consulting Services","p":["The ACE offers a variety of support and consulting services to help organizations effectively utilize data analytics in their operations. These services may include:","Developing a data analytics strategy that aligns with business goals and objectives, and outlining the roles, responsibilities, and processes required for its implementation.","Providing training and education to enhance employees' data literacy, visualization, and analysis skills.","Establishing best practices and frameworks for data analytics, including approaches to data collection, analysis, and visualization, and standardizing data analytics processes across the organization.","Managing data analytics projects, including defining project scope and objectives, managing resources, and tracking progress and outcomes.","Offering expert guidance and consultation on data analytics projects, including identifying appropriate data sources, developing analysis plans, and providing technical support.","The ACE is a valuable resource for organizations seeking to effectively utilize data analytics to achieve their objectives."]},{"l":"Best Practices and Standards in Data Analytics","p":["The ACE plays a crucial role in helping organizations to adopt and adhere to best practices and standards in data analytics. Best practices are generally accepted guidelines or approaches that are considered to be the most effective or efficient way of achieving a particular goal, while standards provide a basis for comparison or evaluation and can help to ensure that data analytics projects and initiatives are aligned with industry standards.","By providing guidance on best practices and standards management, the ACE can help organizations to ensure that their data analytics initiatives are based on a solid foundation of technical, methodological, and organizational principles. This may involve providing training and education on best practices and standards, developing and promoting guidelines and frameworks for data analytics, and helping to identify and implement appropriate standards and protocols for data analytics projects and initiatives.","Adopting and adhering to best practices and standards can help organizations to more effectively leverage data analytics to support their mission and objectives, and to ensure that their data analytics initiatives are aligned with industry standards and best practices. By working with the ACE, organizations can ensure that they have the necessary tools, resources, and expertise to effectively manage their data analytics initiatives and achieve their desired outcomes."]},{"l":"Data Analytics Training and Enablement Services","p":["The ACE is responsible for providing training and enablement services to help organizations develop the skills and knowledge they need to effectively use data analytics to support their mission and objectives. This may include in-person training sessions, online courses and workshops, webinars, and other educational resources. The content and format of the training and enablement services will be tailored to the needs and goals of the organization and the level of expertise of the employees receiving the training. By providing these services, the ACE helps organizations to improve their data literacy, enhance their data analysis and visualization skills, and become more proficient in using data analytics to drive business decisions and outcomes."]}],[{"l":"Developing a Data Analytics Roadmap","p":["Identifying the path forward is a crucial step in any data analytics project or initiative. It involves developing a clear and actionable roadmap or plan for achieving your defined goals and objectives, and involves the following steps:","By following these steps, you can ensure that you have a clear and actionable plan in place for achieving your data analytics goals and objectives."]},{"l":"Define your project or initiative goals and objectives","p":["Clearly define what you want to achieve with your data analytics project or initiative, and how you will measure success."]},{"l":"Develop a communication plan","p":["Establish a comprehensive communication plan to ensure that stakeholders and sponsors are kept informed of project risks, milestones, timelines, and blockers."]},{"l":"Create a roadmap or plan","p":["Based on your defined goals and objectives, develop a roadmap or plan for achieving them, including identifying key tasks and milestones, and allocating resources and responsibilities."]},{"l":"Monitor progress","p":["Regularly monitor progress and other metrics related to the project or program to ensure that it is on track and meeting your defined goals and objectives."]},{"l":"Review and adjust","p":["Regularly review your metrics and the performance and impact of the program, and be prepared to pivot or adjust the plan as needed to ensure success."]}],[{"l":"The Data-Driven Product Development Process","p":["An MVP (minimum viable product) is a product that has the minimum set of features and functionality needed to satisfy end users, and to provide a basis for learning and future development.","As end users come to these office hours and talk to one another as well as the ACE, they can begin to share their lessons learned and experiences with one another. This helps build and foster community and trust which ultimately creates culture.","Create a prototype.","Define the persona and user group.","Define the problem that the persona is trying to solve and determine the applicable KPIs or insights to include in the MVP.","Design and develop the dashboard or schedule reports.","Finally, iterate and evolve. Use the data and feedback gathered from the MVP to refine and improve the product, and to add new features and functionality over time. Don’t forget to communicate with your user community along the way!","From MVP to Iteration","I like to use the MVP concept to describe what the end user should expect out of the gate. It is tempting to think that every KPI and visualization a user could want is on their dashboard out of the box but as we all know, there is some assembly required.","Identify key features like filtering or the ability to select from a dropdown list.","If we’ve already created an IT Service Blueprint for one of the organization’s services, we can begin to ask our personas questions about what KPIs are going to be most meaningful and important off the bat. This could be a measurement of capacity or speed. The bare bones of a dashboard are assembled to create the MVP for this persona and over the next months or quarters, it will be up to the ACE to solicit and collect feedback from the end user community.","Launch the MVP! Make it available to users and begin gathering data and feedback to inform future development.","Let’s summarize some typical steps when launching an MVP:","Planning for Ask Me Anything sessions or Office Hours can be a good way to make the ACE a welcoming place.","Testing and validation of the MVP.","The goal of launching an MVP is to quickly and efficiently test and validate a product concept, and to gather insights and feedback that can inform future development."]}],[{"l":"Getting Tactical"},{"l":"Navigation","p":["Recommendations","Essentials in Analytics Operations","Phased Approach to Security Maturity","The Role of Threat Intelligence in Cybersecurity Maturity","Risk: Assessments, Modifiers, and Variables","How Splunk Enhances All Functional Groups in the SOC","Increase Access, Throughput, and Value From Data via Splunk"]}],[{"l":"Recommendations","p":["These are my tactical recommendations for you to consider when thinking about the essentials in security analytics and security operations:","These are some examples of practical recommendations you can consider in order to approach the essentials in security analytics and security operations. It is important to tailor your approach to the specific needs of your organization, and to be proactive in identifying and addressing potential threats."]},{"l":"Create a data-driven security strategy","p":["By focusing on data-driven approaches to security, organizations can more effectively identify, prioritize, and address security threats and vulnerabilities. This may involve establishing key performance indicators (KPIs) to measure the effectiveness of security efforts, and using analytics to identify trends and patterns in security data."]},{"l":"Develop a strong security analytics team","p":["Building a team of skilled security analysts is critical for successful security analytics. This team should have expertise in areas such as data analysis, visualization, and machine learning, and should be equipped with the right tools and technologies to support their work."]},{"l":"Leverage automation and machine learning","p":["Automation and machine learning can help to streamline and improve the efficiency of security analytics and operations. For example, machine learning algorithms can be used to identify patterns and trends in security data, while automation can help to reduce the workload of security analysts by automating routine tasks."]},{"l":"Foster collaboration and communication","p":["Effective security analytics and operations require strong collaboration and communication between different teams and stakeholders. This may involve establishing clear lines of communication between the security analytics team and other teams within the organization, as well as establishing effective channels for sharing security data and insights."]},{"l":"Invest in the right tools and technologies","p":["In order to support effective security analytics and operations, organizations need to invest in the right tools and technologies. This may include security analytics platforms, visualization tools, and machine learning algorithms, as well as technologies to support data collection and management."]},{"i":"implement-a-security-information-and-event-management-siem-system","l":"Implement a security information and event management (SIEM) system","p":["A SIEM system can help to collect, analyze, and report on security-related events and data from a variety of sources, including network devices, servers, applications, and security devices. This can help to provide visibility into potential threats and help you to identify and respond to incidents more quickly."]},{"i":"implement-a-security-orchestration-automation-and-response-soar-platform","l":"Implement a security orchestration, automation, and response (SOAR) platform","p":["A SOAR platform can help to automate and streamline your incident response process by providing tools for automating incident triage, escalating incidents to the appropriate teams, and coordinating response efforts."]},{"l":"Implement a vulnerability management program","p":["A vulnerability management program can help you to identify and prioritize vulnerabilities in your systems and applications, and implement measures to mitigate or eliminate those vulnerabilities. This can help to reduce your risk of successful attacks."]},{"l":"Implement a threat intelligence program","p":["A threat intelligence program can help you to stay informed about emerging threats and trends, and provide guidance on how to mitigate or eliminate those threats. This can help you to proactively protect your organization against potential attacks."]},{"l":"Implement a continuous monitoring program","p":["A continuous monitoring program can help you to regularly assess and monitor your security posture, identify potential vulnerabilities or risks, and implement corrective actions to address those issues. This can help you to maintain a strong security posture over time."]},{"l":"Identify and prioritize the data sources that will be most valuable for your security analytics efforts","p":["This may include network logs, security event logs, and other data sources that can provide insights into potential security threats."]},{"i":"develop-a-plan-for-collecting-storing-and-analyzing-your-security-data","l":"Develop a plan for collecting, storing, and analyzing your security data","p":["This may include implementing a data lake or other data management solution, as well as identifying the tools and technologies that will be needed to analyze and interpret the data."]},{"l":"Implement processes and procedures for responding to security events and incidents","p":["This may include establishing a security operations center (SOC) or other incident response team, as well as defining protocols for responding to different types of security threats."]},{"l":"Train and enable your security analytics and operations teams","p":["Ensure that they have the necessary skills and knowledge to effectively analyze and respond to security threats, and provide ongoing training and support to help them stay up to date on the latest threats and technologies."]},{"l":"Regularly review and optimize your security analytics and operations efforts","p":["Use metrics and other performance indicators to identify areas for improvement, and make adjustments as needed to ensure that you are effectively addressing security threats and meeting your objectives."]}],[{"l":"Essentials in Analytics Operations"},{"l":"Data"},{"l":"Data Storage Options for Business Analytics","p":["Data storage is a critical component of any business analytics strategy. A data warehouse is a repository of data that is organized and structured to support business analysis. However, there are other options available, such as data lakes and reservoirs, which offer different capabilities and features. It is important for businesses to choose the data storage option that best fits their needs and goals. Some considerations to keep in mind include the ability to access data from various locations, the ability to integrate with different analytics platforms, and the overall cost and scalability of the solution. In order to ensure that your business can effectively leverage data analytics, it is essential to carefully evaluate and choose the right data storage option."]},{"l":"The Importance of Data Management in a Data Strategy","p":["Effective data management is crucial for any organization looking to leverage data to drive business success. It involves the planning and execution of processes and systems that ensure the accurate, accessible, and usable storage of data. This includes activities such as data collection and storage, data integration, data cleansing and quality management, data security, and data governance. A well-designed data management system is necessary for supporting the use of data to inform decision-making and drive business actions, and is an essential component of a successful data strategy. Therefore, it is important to consider data management as a key part of any data strategy planning process"]},{"l":"The Importance of Data Architecture in a Data Strategy","p":["Data architecture is a crucial aspect of a data strategy, as it determines the structure and organization of the data system or ecosystem that supports the data strategy. It includes the design of data models, the relationships between data elements, and the physical implementation of data storage and processing systems. A well-designed data architecture is essential for ensuring that data is accurate, accessible, and usable within an organization, and is a key factor in the successful implementation of a data strategy.","There are two main categories of data architecture: logical data architecture, which defines the logical structure and relationships of data, and physical data architecture, which defines the physical structure and implementation of data storage and processing systems. Both are important considerations when designing and implementing a data strategy.","Effective data architecture plays a critical role in supporting the effective management and use of data within an organization. It helps to ensure that data is integrated, stored, and secured in a way that supports the organization's business goals, and enables the effective use of data to inform decision-making and drive business actions. As such, data architecture should be an integral part of any data strategy planning process."]},{"l":"Data Architecture Frameworks","p":["Data architecture frameworks are models or frameworks that provide a structure and approach for designing and organizing a data architecture. These frameworks typically include guidelines and best practices for areas such as data modeling, data storage, data integration, data governance, and data security.","There are multiple data architecture frameworks that organizations can choose from, including:","Data architecture frameworks provide a structured approach to designing and organizing a data architecture and can be helpful for organizations that are looking to improve the management and use of their data.","The Data Management Body of Knowledge (DAMA-DMBOK2) is a data management framework and reference guide created by DAMA International, a professional association for data managers.","TOGAF was created in 1995. It is an enterprise architecture framework and methodology that includes a section on data architecture design and roadmap development.","Meant to serve as the basis for an architecture, the Zachman Framework is an ontology framework that uses a 6-x-6 matrix of rows and columns to describe an enterprise architecture and data elements but does not include an implementation methodology.","Using ODAM, organizations can ensure that data is flexible, adaptable, and elastic. ODAM provides the foundational components of a data strategy that your organization can customize and tailor to your business and specific needs. Components built-in to ODAM include:","A guide to implementing a modern data architecture.","Assistance in setting up agile data governance practices.","Assistance in adopting and utilizing data integration and transformation tools.","Guidance on implementing data virtualization.","A framework to set up regular monitoring and optimization of data performance.","These strategies, when implemented in combination, can help organizations to improve the flow and use of data, making it more scalable and responsive to changing business needs."]},{"l":"The Zachman Framework","p":["This framework provides a structured approach to data architecture design by considering six dimensions of data: who, what, where, when, why, and how."]},{"l":"The Data Vault Model","p":["This framework is based on a hub-and-spoke model and is designed to support large, complex data architectures that need to be flexible and scalable."]},{"l":"The Data Warehouse Bus Architecture","p":["This framework is designed to support data warehousing and business intelligence systems and is based on a bus-based data model."]},{"l":"The Data Governance Framework","p":["This framework is focused on the governance and management of data and includes guidelines and best practices for areas such as data ownership, data quality, and data security."]},{"l":"Maintaining Data Quality","p":["Maintaining data quality is the process of ensuring that data is accurate, complete, and up-to-date. This includes verifying the accuracy of data, correcting errors or inconsistencies, and removing or de-duplicating duplicate or outdated data. Data quality is an important aspect of data management, as it helps to ensure that data is reliable and can be used effectively for business purposes.","Poor data quality can lead to a variety of problems, such as incorrect or incomplete insights, wasted time and resources, and damage to an organization's reputation. To maintain data quality, it is important to establish and implement processes for regularly reviewing and cleaning data, as well as for verifying the accuracy of data as it is collected and entered into systems.","In terms of data architecture, data quality should be considered when designing and implementing data storage and processing systems. This may include implementing data validation and error correction mechanisms, as well as regularly reviewing and cleaning data to ensure that it is accurate and up-to-date. Maintaining data quality is an ongoing process that requires ongoing attention and resources, and should be an integral part of an organization's data management strategy."]},{"i":"it-assets-networks-applications-and-users","l":"IT Assets, Networks, Applications, and Users","p":["Monitoring IT assets is a crucial part of managing and maintaining a strong and secure technology infrastructure. By monitoring IT assets, organizations can identify and address issues as they arise, and ensure that their systems are functioning optimally.","Continuously monitoring the IT network for threats can provide a range of benefits to organizations. By continuously monitoring the IT network for threats, organizations can identify and address potential vulnerabilities or threats, enhance their threat visibility, improve their incident response capabilities, and reduce risk.","Continuously monitoring applications can provide a range of benefits to organizations. By continuously monitoring applications, organizations can identify and address issues that may be impacting system performance, enhance security, improve incident response capabilities, and reduce risk.","Organizations can anticipate increased user productivity and security, maintain regulatory compliance, protect sensitive data and prevent data breaches, and speed up incident response time by continuously monitoring users, identities, and access."]}],[{"l":"Phased Approach to Security Maturity","p":["\"Data is like garbage. You’d better know what you are going to do with it before you collect it.\"— Mark Twain","Splunk's phased approach to security maturity is a systematic framework that helps organizations to progressively improve their security posture over time. The approach is designed to help organizations to quickly detect, investigate, and respond to threats.","Ad-hoc search and investigation capabilities (Level 1).","Proactive Monitoring and Alerting (Level 2).","Security Situational Awareness (Level 3).","Real-time Risk Insight and Automation (Level 4).","One of the key benefits of ODAM is its ability to increase visibility into security logs and enable organizations to monitor their security posture in real-time. This can help organizations identify potential security threats more quickly and respond to them more effectively. Additionally, ODAM can provide advanced security analytics and a risk-based alerting framework, which can help organizations proactively identify and respond to security incidents.","ODAM can also help organizations integrate threat intelligence into their security operations. By incorporating threat intelligence into their data analytics initiatives, organizations can enhance their overall security posture and stay ahead of potential threats.","One of the most exciting ways that ODAM can help organizations is by preparing for new capabilities like Splunk Mission Control. Splunk Mission Control is an innovative platform that unifies security operations and enables organizations to improve their incident response times, automate their workflows, and gain real-time visibility into their security posture.","By using ODAM to mature and operationalize your data analytics capabilities, organizations can be better prepared to take advantage of new technologies like Splunk Mission Control. This can help improve threat detection, investigation, and response capabilities, and ultimately become more resilient against security threats."]}],[{"l":"The Role of Threat Intelligence in Cybersecurity Maturity","p":["Threat intelligence is essential in a phased approach to achieving cybersecurity maturity. It enables organizations to understand the various types of threats they may encounter and the tactics, techniques, and procedures (TTPs) that attackers use. This information helps organizations to develop a comprehensive cybersecurity strategy that addresses their specific threats and vulnerabilities.","During the planning phase, threat intelligence is used to identify potential threats and vulnerabilities and prioritize the most pressing risks. This information is then used to inform the development of a risk assessment and risk management plan.","In the implementation phase, threat intelligence helps organizations to select and deploy appropriate cybersecurity controls or technologies.","During the ongoing maintenance phase, threat intelligence is used to continuously monitor for and assess new threats and vulnerabilities, and to update the organization's cybersecurity strategy and controls as needed.","In summary, threat intelligence is a crucial factor in a phased approach to achieving cybersecurity maturity, as it provides organizations with the information they need to identify and prioritize threats, and to implement and maintain a robust and effective cybersecurity strategy."]}],[{"i":"risk-assessments-modifiers-and-variables","l":"Risk: Assessments, Modifiers, and Variables"},{"l":"Risk Assessments","p":["Risk assessments are an important tool for determining company priorities. Organizations may better understand the risks they face and determine the most important priorities for resolving those risks by examining the effect, exposure, threats, and likelihood of risk occurrences. Finally, risk assessments assist companies in making educated decisions about how to deploy resources and reduce possible risks in order to safeguard assets and meet business objectives."]},{"l":"Risk Factor Editor","p":["In today's digital world, managing cyber risk is an essential part of any organization's operations. One important tool for doing so is the Risk Factor Editor, which allows organizations to take a range of considerations into account when evaluating and prioritizing their cyber risk. Some of the key considerations that the risk factor editor can factor in include: asset criticality, alert criticality, SLA classification, data classification, application criticality, network criticality, user criticality, and MITRE TTP details. In this section, we will delve into each of these risk factors in more detail, exploring what they are, why they are important, and how they can be used to help organizations manage their cyber risk more effectively. By understanding the full range of factors that can impact an organization's cyber risk, IT professionals can make more informed and strategic decisions about how to protect their systems, data, and users from potential threats.","ODAM helps organizations pinpoint risk factors and make adjustments to their risk evaluation and policy enforcement processes. By using ODAM, organizations can take a targeted approach to risk management and streamline their processes for identifying, assessing, and addressing risks in real-time. ODAM can be a valuable tool for organizations responsible for managing and mitigating cyber risk."]},{"l":"Asset Criticality","p":["This refers to the importance or value of a particular asset to the organization, such as a server, database, or application. An asset's criticality may be determined by factors such as the impact it has on the organization's operations, the cost of replacing it, or the sensitivity of the data it stores."]},{"l":"Alert Criticality","p":["This refers to the level of urgency or importance of an alert or notification, such as a security breach or system failure. An alert's criticality may be determined by the potential impact on the organization's operations, the likelihood of damage or loss, or the time required to address the issue."]},{"l":"SLA Classification","p":["This refers to the classification or categorization of a service level agreement (SLA), which is a contract that outlines the terms and conditions for delivering a specific service. An SLA classification might include categories such as standard, premium, or critical, depending on the level of service required."]},{"l":"Data Classification","p":["This refers to the process of categorizing data based on its sensitivity, value, or importance to the organization. Data classification may be used to determine the level of protection required, the access controls needed, or the legal or regulatory obligations associated with the data."]},{"l":"App Criticality","p":["This refers to the importance or value of a particular application to the organization, such as a customer relationship management system or an enterprise resource planning system. An app's criticality may be determined by factors such as the impact it has on the organization's operations, the cost of replacing it, or the sensitivity of the data it stores."]},{"l":"Network Criticality","p":["This refers to the importance or value of a particular network or network component to the organization, such as a server, switch, or router. A network's criticality may be determined by factors such as the impact it has on the organization's operations, the cost of replacing it, or the sensitivity of the data it transmits."]},{"l":"User Criticality","p":["This refers to the importance or value of a particular user or user group to the organization, such as employees, customers, or partners. A user's criticality may be determined by factors such as their role within the organization, their level of access to sensitive data or systems, or their impact on the organization's operations."]},{"l":"TTP Details","p":["Tactic, technique, and procedure (TTP) details are important risk variables to consider when analyzing and mitigating cyber threats. These details refer to the specific methods or approaches used to carry out an action, such as a cyber attack or security breach. By understanding TTP details, organizations can identify patterns or trends in attack methods and understand the potential risks and impacts associated with these methods. This can inform risk management and security strategies, and help to develop more effective countermeasures to protect against potential threats."]}],[{"l":"How Splunk Enhances All Functional Groups in the SOC","p":["A mature Security Operations Center (SOC) typically includes the following functional groups:","Compliance: This group is responsible for ensuring that the organization is compliant with relevant regulations and standards, such as PCI DSS, HIPAA, and NIST.","Enhancement with Splunk","Risk Management: This group is responsible for identifying, assessing, and mitigating risks to the organization's assets, including data, systems, and networks.","Security Architecture: This group is responsible for designing, implementing, and maintaining the security infrastructure of the organization.","Security Engineering: This group is responsible for implementing, maintaining, and updating security controls and technologies.","Security Operations: This group is responsible for monitoring and analyzing security events and incidents, identifying threats, and taking action to mitigate them.","SOC Functional Group","Splunk can help compliance teams to ensure that the organization is compliant with relevant regulations and standards. Splunk can provide real-time visibility into the state of an organization's security posture, and can help teams to identify and address areas of non-compliance.","Splunk can help risk management teams to identify, assess, and mitigate risks to the organization's assets, including data, systems, and networks. Splunk can provide real-time visibility into the state of an organization's security posture, and can help teams to identify and prioritize risks based on their potential impact and likelihood of occurrence.","Splunk can help security architecture teams to design, implement, and maintain the security infrastructure of the organization. Splunk can provide real-time visibility into the state of an organization's security infrastructure, and can help teams to identify and address weaknesses and vulnerabilities.","Splunk can help security engineering teams to implement, maintain, and update security controls and technologies. Splunk can provide real-time visibility into the state of an organization's security controls, and can help teams to identify and address weaknesses and vulnerabilities.","Splunk can help threat intelligence teams to gather, analyze, and disseminate intelligence about threats, vulnerabilities, and trends in the cyber landscape. Splunk can provide a centralized platform for storing and analyzing intelligence data, and can help teams to identify patterns and trends that may indicate the presence of a new or evolving threat.","Splunk can help vulnerability management teams to identify and prioritize vulnerabilities, and to monitor progress in addressing them. Splunk can provide real-time visibility into the state of an organization's security posture, and can help teams to identify and prioritize vulnerabilities based on their potential impact and likelihood of exploitation.","Splunk can provide real-time visibility into security events and incidents, allowing security operations teams to quickly identify and respond to threats. Splunk can also provide analytics capabilities to help security operations teams understand the nature and scope of threats, and to identify patterns and trends that may indicate the presence of a larger attack.","Threat Intelligence: This group is responsible for gathering, analyzing, and disseminating intelligence about threats, vulnerabilities, and trends in the cyber landscape.","Vulnerability Management: This group is responsible for identifying and prioritizing vulnerabilities, coordinating remediation efforts, and monitoring progress."]},{"l":"Continuous Monitoring for Cybersecurity","p":["Threat Intelligence, Incident Handling, Forensics, and Self-Assessment","\"Not everything that can be counted counts and not everything that counts can be counted.\"— Albert Einstein, Physicist","The focus should not be on a specific data source or technology but rather on the broader concept of observability.","IT operations refer to the activities and processes involved in maintaining and managing the technology infrastructure, systems, and applications of an organization. This can include tasks such as installing and configuring hardware and software, monitoring systems and networks for issues or outages, and troubleshooting and resolving problems as they arise.","IT observability, on the other hand, refers to the ability to monitor, measure, and understand the performance and behavior of IT systems, networks, and applications. This can include collecting and analyzing data from various sources, such as log files, performance metrics, and traffic patterns, to identify trends, issues, and potential problems.","The main difference between IT operations and IT observability is that IT operations focus on the day-to-day management and maintenance of IT systems, while IT observability focuses on understanding and optimizing the performance and behavior of these systems. Both are important for ensuring the smooth and reliable operation of an organization's technology infrastructure, but they serve different purposes and use different approaches and tools.","Threat intelligence, incident handling, forensics, and self-assessment can all be important factors in a continuous monitoring approach to cybersecurity.","Continuous monitoring with respect to cybersecurity involves continuously collecting and analyzing data from various sources, such as threat intelligence feeds, security logs, and network traffic, to identify potential risks and vulnerabilities, and to respond to and manage incidents or breaches as they arise. By using tools and processes such as threat intelligence, incident handling, forensics, and self-assessment, organizations can improve their ability to identify and mitigate potential threats, and to maintain a strong security posture.","Cybersecurity Operations and Monitoring teams running a Security Operations Center (SOC) require data analytics at operational speed."]},{"l":"Threat intelligence","p":["Threat intelligence refers to information about potential threats or vulnerabilities that an organization may face, such as new malware strains or zero-day exploits. This information can be used to inform continuous monitoring efforts by helping to identify potential risks and vulnerabilities, and to prioritize the most important areas to focus on."]},{"l":"Incident handling","p":["Incident handling refers to the process of responding to and managing security incidents or breaches, such as malware infections or unauthorized access to sensitive data. This can be an important part of continuous monitoring because it helps to identify and address potential issues as they arise, and to prevent further damage or loss."]},{"l":"Forensics","p":["Forensics refers to the process of collecting, analyzing, and preserving digital evidence for the purpose of investigating security incidents or crimes. This can be an important part of continuous monitoring because it helps to identify the root causes of incidents or breaches, and to understand the extent of the damage or loss."]},{"l":"Self-assessment","p":["Self-assessment refers to the process of evaluating an organization's security posture and identifying areas for improvement. This can be an important part of continuous monitoring because it helps to identify weaknesses or vulnerabilities that may not be immediately apparent, and to take steps to address them."]},{"l":"Benefits of Constant Introspection for IT Organizations","p":["Constant introspection refers to the process of continuously monitoring and analyzing the performance and behavior of an organization's technology infrastructure, systems, and applications. This practice can provide numerous benefits for IT organizations, including:","By implementing constant introspection, IT organizations can optimize the performance and efficiency of their systems, improve security, and ensure compliance."]},{"l":"Performance monitoring","p":["Identifying potential bottlenecks or issues that may be impacting system performance."]},{"l":"Capacity planning","p":["Understanding resource requirements and planning for future capacity needs."]},{"l":"Security monitoring","p":["Identifying potential vulnerabilities or threats."]},{"l":"Compliance monitoring","p":["Ensuring compliance with relevant regulations and standards."]},{"l":"Root cause analysis","p":["Identifying the root causes of problems and addressing them."]},{"l":"Tracking Key Performance Indicators for IT Systems and Applications"},{"l":"Performance Monitoring","p":["Performance monitoring and capacity planning are important activities for ensuring the smooth and reliable operation of an organization's technology infrastructure, systems, and applications. Some specific key performance indicators (KPIs) that an organization can track when it comes to these activities include:","Tracking these and other KPIs can help organizations to understand the performance and capacity of their IT systems and applications, and to identify potential issues or problems that may need to be addressed."]},{"l":"System availability","p":["This KPI measures the percentage of time that a system is available and functioning as expected. This can be important for ensuring that systems are meeting the needs of users and customers."]},{"l":"Response time","p":["This KPI measures the time it takes for a system or application to respond to a request. This can be important for ensuring that systems are responsive and perform well."]},{"l":"Throughput","p":["This KPI measures the amount of data or transactions that a system or application can process in a given time period. This can be important for understanding the capacity and performance of a system."]},{"l":"Error rate","p":["This KPI measures the percentage of requests or transactions that result in errors. This can be important for identifying potential issues or problems with a system."]},{"l":"Resource utilization","p":["This KPI measures the percentage of available resources (such as CPU, memory, or storage) that are being used by a system or application. This can be important for understanding capacity and performance, and for identifying potential bottlenecks or issues."]},{"l":"Capacity utilization","p":["This KPI measures the percentage of available capacity (such as bandwidth or storage) that is being used by a system or application. This can be important for understanding capacity and performance, and for identifying potential bottlenecks or issues."]},{"l":"Compliance Monitoring","p":["Compliance monitoring and root cause analysis are important activities for ensuring that an organization's technology infrastructure, systems, and applications are operating in a compliant and effective manner. Some specific key performance indicators (KPIs) that an organization can track when it comes to these activities include:","Tracking these and other KPIs can help organizations to understand the compliance and reliability of their IT systems and applications, and to identify and address potential issues or problems that may need to be addressed."]},{"l":"Compliance rate","p":["This KPI measures the percentage of transactions or processes that are compliant with relevant regulations and standards. This can be important for ensuring that an organization is meeting its compliance obligations."]},{"l":"Number of compliance violations","p":["This KPI measures the number of instances in which an organization's systems or processes are found to be non-compliant. This can be important for understanding the extent to which an organization is meeting its compliance obligations."]},{"l":"Time to compliance","p":["This KPI measures the time it takes for an organization to bring its systems or processes into compliance after a violation has been identified. This can be important for ensuring that issues are addressed promptly and effectively."]},{"l":"Compliance cost","p":["This KPI measures the financial cost of maintaining compliance with relevant regulations and standards. This can be important for understanding the resources that are required to meet compliance obligations."]},{"l":"Root cause resolution rate","p":["This KPI measures the percentage of problems or issues that are successfully addressed at the root cause level. This can be important for ensuring that issues are not just being patched over, but are being fully resolved."]},{"i":"mean-time-to-resolution-mttr","l":"Mean time to resolution (MTTR)","p":["This KPI measures the average time it takes to resolve a problem or issue. This can be important for understanding the efficiency and effectiveness of an organization's problem-solving processes."]},{"i":"mean-time-between-failures-mtbf","l":"Mean time between failures (MTBF)","p":["This KPI measures the average time between failures or issues with a system or application. This can be important for understanding the reliability and stability of a system."]},{"i":"mean-time-to-detect-mttd","l":"Mean time to detect (MTTD)","p":["This KPI measures the average time it takes to detect a problem or issue. This can be important for identifying and addressing problems as soon as possible."]},{"l":"Incident Response","p":["Incident response is the process of managing and responding to security incidents or other disruptions that may impact an organization's technology infrastructure, systems, or applications. Some specific key performance indicators (KPIs) that an organization can track when it comes to incident response include:","Tracking these and other KPIs can help organizations to understand the effectiveness of their incident response processes, and to identify areas where improvements may be needed."]},{"i":"mean-time-to-identify-mtti","l":"Mean time to identify (MTTI)","p":["This KPI measures the average time it takes to identify a security incident or other disruption. This can be important for identifying and addressing issues as soon as possible."]},{"i":"mean-time-to-respond-mttr","l":"Mean time to respond (MTTR)","p":["This KPI measures the average time it takes to respond to a security incident or other disruption. This can be important for ensuring that issues are addressed promptly and effectively."]},{"i":"mean-time-to-recovery-mttr","l":"Mean time to recovery (MTTR)","p":["This KPI measures the average time it takes to recover from a security incident or other disruption. This can be important for ensuring that systems are restored and operational as quickly as possible."]},{"l":"Number of incidents","p":["This KPI measures the total number of security incidents or disruptions that an organization experiences over a given time period. This can be important for understanding the frequency and impact of such incidents."]},{"l":"Incident severity","p":["This KPI measures the severity or impact of security incidents or disruptions on an organization. This can be important for understanding the potential impact of such incidents and for prioritizing response efforts."]}],[{"i":"increase-access-throughput-and-value-from-data-via-splunk","l":"Increase Access, Throughput, and Value From Data via Splunk","p":["To achieve this, organizations need to understand the metrics that make them tick, leverage the data to understand changes in their environment early, and be quick to take action.","Here are four recommendations for organizations to help them understand their own key performance indicators (KPIs) or metrics:","Understanding your own KPIs and metrics is an important step in leveraging data to understand changes in your environment and take quick action. By identifying the key business objectives, determining the most relevant metrics, establishing baseline metrics, and monitoring and analyzing metrics regularly, organizations can increase access, throughput, and value from their data."]},{"l":"1. Identify the key business objectives","p":["The first step in understanding your own KPIs is to identify the key business objectives that the organization is trying to achieve. This will help to ensure that the KPIs chosen are aligned with the goals of the organization and are focused on delivering value."]},{"l":"2. Determine the most relevant metrics","p":["Once the key business objectives have been identified, the next step is to determine the most relevant metrics that can be used to track progress towards those objectives. This may involve considering a range of different metrics, such as financial metrics, operational metrics, customer metrics, or employee metrics."]},{"l":"3. Establish baseline metrics","p":["In order to track progress over time, it is important to establish baseline metrics that can be used as a reference point. This may involve collecting data on the current state of the organization, so that changes can be measured and tracked over time."]},{"l":"4. Monitor and analyze metrics regularly","p":["Once the relevant metrics have been identified and baseline data has been collected, it is important to monitor and analyze the metrics on a regular basis. This may involve using tools such as Splunk to visualize and analyze the data, and to identify trends, patterns, or issues that may need to be addressed."]}],[{"l":"Conclusion","p":["In this book, we have discussed the importance of data analytics in helping organizations make informed and strategic decisions. We have explored the use of tools like ODAM platform and emphasized the importance of data storytelling and establishing a common language around data. We have also outlined steps for setting up an analytics center of excellence and emphasized the need to take a tactical approach to deployment.","Ultimately, Splunk connects people to what they are looking for and ODAM connects people, processes, technology, and data.","I will leave you with these last-minute thoughts:","If companies want to scale, data must be an asset for all, not an asset for just a few. It is hard to scale if all data skills are concentrated in a few people.","Making storytelling, clear, and simple by instilling best practices and reducing cognitive load all while using beautiful design.","Talk to each other, be patient, listen, and help each other out. Everyone is learning at their own speed and in their own way. Practice telling stories with each other.","Thank you for reading. I hope the information and ideas contained in this writing and in ODAM are helpful to you. Please feel free to contact me if you have any feedback (of any kind!).","\"If you wanna do data science, learn how it is a technical, cultural, economic, and social discipline that has the ability to consolidate and rearrange societal power structures.\"— Hugo Bowne-Anderson, Head of Data Science Evangelism and Marketing at Coiled"]},{"i":"a-god-loosened-by-jimmy-santiago-baca","l":":icon-sponsor-tiers: A God Loosened, by Jimmy Santiago Baca","p":["See Video","I walk along the acequia.","Morning quivers softly.","Glassy sunlight sparkles in the yellowed grass.","Sparrow flicks from a tree across marginless blue sky.","All that drips, glows, hollows a bright flash out of the morning.","A flickering chill blows and uplifts flame of this day brighter.","In the upturned claws of great dead eagles that are the snowy woods along the Rio Grande, echoes a crumbling and falling of icy sounds, of logs and trunks cracking in a crackling crush of dead leaves.","The wings of a hawk storm from tangled boughs, wings woop fiercely in above-ground canyons of branches, drumming the air, and in the frightful break of silence, suddenly, it seems a God has loosened itself in those terrible feathers.","— Jimmy Santiago Baca"]}],[{"l":"About the Author","p":["As a cybersecurity consultant and architect with over 17 years of experience, I have a passion for helping organizations secure their lines of business, innovate using data analytics, and improve their overall security posture. My expertise spans multiple technology and functional domains, and I have a strong track record of designing, delivering, and implementing new technologies for clients in various industries, including higher education, healthcare, government, utilities, and finance.","I am a trusted advisor and client success advocate, always striving to earn exceptional feedback from stakeholders. In addition to my professional work, I am deeply committed to giving back to my community through volunteering. I am a Microsoft TEALs volunteer computer science teacher at Santa Teresa High School and a Technical Mentor in the CyberPatriots program, and I always look for ways to help mentees along the way.","My key focus areas in recent years have been K-12 and election infrastructure security, but I also enjoy working with clients in a variety of sectors. I am currently working on a groundbreaking project called ODAM (Operationalizing Data Analytics Methodology) which aims to assist organizations of all kinds with operationalizing data analytics and driving value from data.","I have an interest in the ways that systems and people interact, and am always seeking new ways to engage with an audience through marketing, branding, and experience. In addition to my professional work, I am passionate about blockchain, cryptocurrency, and NFTs, and enjoy staying up-to-date with the latest developments in these fields.","I am skilled at creating frameworks and methodologies to help organizations adopt new technologies and achieve their business goals. This includes developing strategies for cross-functional collaboration and mentoring, as well as aligning technology with business objectives in order to drive meaningful results and solve complex problems. I have a talent for working with clients to understand their unique needs and challenges, and am dedicated to delivering high-quality solutions that meet their needs and exceed their expectations.","As I continue to grow and develop in my career, my goal is to help my clients achieve success through the effective use of technology, while also staying up-to-date with the latest trends and best practices in my field. I am always open to new challenges and opportunities to make a positive impact in the world of cybersecurity and in my community."]}]]